{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6735a9ec",
   "metadata": {},
   "source": [
    "# Stage 1: The Baseline RAG Agent\n",
    "\n",
    "Now that you've established some working knowledge of system context, it's time to start exploring our agent's capabilities and, more importantly, its limitations. More specifically, we'll do the following:\n",
    "\n",
    "1.  Explore the baseline architecture of the simple RAG agent. Our Stage 1 Agent takes the simplest possible approach: Find relevant courses and give the LLM *everything* about them.\n",
    "2.  Witness \"Information Overload\": See firsthand what happens when you retrieve *too much* context.\n",
    "3.  Analyze the Cost: Measure the token usage of a naive approach.\n",
    "\n",
    "Let's now dive in by going over how the agent works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e610a6e9",
   "metadata": {},
   "source": [
    "## Agent Overview\n",
    "\n",
    "First, an overview of the code. If you want to explore on your own, the baseline agent lives in the `progressive_agents/stage1_baseline_rag/` directory. You can reference the code at any time throughout this lesson.\n",
    "\n",
    "We are using the following technologies in the stack:\n",
    "\n",
    "- **LangGraph**: The orchestrator. It manages the control flow of our application, defining how data moves between steps (nodes). If you're not familiar with the components of LangGraph, we recommend you check out their overview on [\"Thinking in LangGraph\"](https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph).\n",
    "- **Redis**: Our datastore. It serves as our vector database, allowing us to perform semantic search to find relevant courses.\n",
    "- **LangChain**: The connector. It provides the standard interfaces for interacting with LLMs and prompts.\n",
    "- **OpenAI (GPT-4o-mini)**: Our reasoing model\n",
    "\n",
    "> **A note on architecture**: In a real-world scenario, a simple RAG pipeline doesn't necessarily require a full \"agent\" architecture with state management and graph orchestration. A simple function chain would often suffice. However, for this course, we have implemented it as a basic Agent using LangGraph. This allows us to introduce the core components (state, nodes, workflow) from the start, providing a solid foundation for adding complexity, such as memory, tools, and decision-making, in later stages.\n",
    "\n",
    "Note the agent has three important components:\n",
    "\n",
    "### 1. LangGraph Nodes (agent/nodes.py)\n",
    "\n",
    "The logic is split into two functions (nodes):\n",
    "*   `research_node`:\n",
    "    *   Searches Redis for the top 5 courses matching the user's query.\n",
    "    *   Note that it retrieves the FULL hierarchical data for all 5 courses. This includes every single week of the syllabus, every homework assignment, and every reading list.\n",
    "*   `synthesize_node`:\n",
    "    *   Receives the massive block of text about the 5 courses\n",
    "    *   Sends it all to the LLM with a prompt to answer the user's question.\n",
    "\n",
    "### 2. LangGraph State (agent/state.py)\n",
    "\n",
    "We use a `TypedDict` to pass data between nodes.\n",
    "```python\n",
    "class AgentState(TypedDict):\n",
    "    query: str              # The query sent to the LLM\n",
    "    raw_context: str        # The JSON blob of course data retrieved\n",
    "    final_answer: str       # The LLM's response\n",
    "    total_tokens: int       # Tracking token usage\n",
    "```\n",
    "\n",
    "### 3. LangGraph Workflow (agent/workflow.py)\n",
    "\n",
    "We use LangGraph to orchestrate the flow. It's a linear graph:\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    START([Start]) --> Research[Research Node]\n",
    "    Research --> Synthesize[Synthesize Node]\n",
    "    Synthesize --> END([End])\n",
    "    \n",
    "    style Research fill:#ff9999,stroke:#333,stroke-width:2px\n",
    "    style Synthesize fill:#99ccff,stroke:#333,stroke-width:2px\n",
    "```\n",
    "\n",
    "\n",
    "Let's now set up the agent and jump into seeing the context we are working with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344c64d1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Just like before, we'll need to import the agent code from the `progressive_agents` directory. Run the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0db6228",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code sets up the notebook to be able to access the provided OpenAI API Key and access to the agent code\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "if \"OPENAI_API_BASE\" in os.environ:\n",
    "    os.environ[\"OPENAI_BASE_URL\"] = os.environ[\"OPENAI_API_BASE\"]\n",
    "\n",
    "project_root = Path(\"..\").resolve()\n",
    "\n",
    "stage1_path = project_root / \"progressive_agents\" / \"stage1_baseline_rag\"\n",
    "src_path = project_root / \"src\"\n",
    "\n",
    "sys.path.insert(0, str(src_path))\n",
    "sys.path.insert(0, str(stage1_path))\n",
    "\n",
    "print('OpenAI API key and agent access setup!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e92bb0b",
   "metadata": {},
   "source": [
    "Again, just like before, we will use the `setup_agent` helper. This function performs a crucial step:\n",
    "*   It connects to your Redis instance.\n",
    "*   It checks if the course data exists.\n",
    "*   If not, it generates 50 sample courses and loads them into Redis.\n",
    "\n",
    "Unlike stage 0, we'll have a more verbose output this time anytime we run the agent. This will help us visualize what the agent is doing more clearly. Run the code block below to start the agent.\n",
    "\n",
    "> ‚ö†Ô∏è **Note**: This might take a few seconds the first time you run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c77682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import setup_agent\n",
    "\n",
    "print(\"Initializing Stage 1 Agent...\")\n",
    "# auto_load_courses=True ensures we have data to query\n",
    "workflow, course_manager = setup_agent(auto_load_courses=True)\n",
    "print(\"Agent is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7356857",
   "metadata": {},
   "source": [
    "## Drowning in Data\n",
    "\n",
    "Imagine a student just wants a quick list of options. They ask:\n",
    "\n",
    "> *\"What computer science courses are available?\"*\n",
    "\n",
    "If this were a human advisor, they would likely reply with something like: *\"We have CS001 (Intro to ML) and CS002 (Deep Learning).\"*\n",
    "\n",
    "But will that be what our agent does? Let's observe. Run the code block below to send the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7c49dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the user's query\n",
    "query = \"What computer science courses are available?\"\n",
    "\n",
    "print(f\"User asks: '{query}'\")\n",
    "print(\"Running workflow...\")\n",
    "\n",
    "# Run the graph!\n",
    "# We use .ainvoke() because our agent is async\n",
    "result = await workflow.ainvoke({\"query\": query})\n",
    "\n",
    "print(\"Workflow complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dc52b3",
   "metadata": {},
   "source": [
    "In most cases, the agent answered the question quite well and provided us with a few courses about computer science. But, pay close attention to the token usage. Let's examine the metrics a bit more closely. \n",
    "\n",
    "Run the code below to see the tokens received by the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e73295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Answer\n",
    "print(\"=\"*60)\n",
    "print(f\"Agent Answer:\\n\\n{result['final_answer']}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Display the Metrics\n",
    "courses_found = result.get('courses_found', 0)\n",
    "total_tokens = result.get('total_tokens', 0)\n",
    "\n",
    "print(f\"\\nStatistics:\")\n",
    "print(f\"   Courses Retrieved: {courses_found}\")\n",
    "print(f\"   Total Tokens Used: {total_tokens:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b02bc2f",
   "metadata": {},
   "source": [
    "Take a look at the total number of tokens. It is likely over 6,000 tokens. This means that for a simple question like *\"What courses are available?\"*, we used enough tokens to write a short essay. Why is it so big? Let's take a closer look at the `raw_context` that was sent to the LLM. \n",
    "\n",
    "Run the code block below to examine the raw context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7590d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the first 2000 characters of the context sent to the LLM\n",
    "raw_context = result.get('raw_context', '')\n",
    "\n",
    "print(f\"Total Context Size: {len(raw_context):,} characters\")\n",
    "print(\"-\" * 40)\n",
    "print(\"PREVIEW OF CONTEXT SENT TO LLM\")\n",
    "print(\"-\" * 40)\n",
    "print(raw_context[:2000] + \"\\n\\n... [TRUNCATED 20,000+ CHARACTERS] ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40886b0c-628f-459d-98ef-a45d04d07ff9",
   "metadata": {},
   "source": [
    "Notice what is in that context:\n",
    "*   `\"week_number\"` that covered the topics in class per week from the first week all the way to the end\n",
    "*   `\"assignments\"` with detailed lists of every homework\n",
    "*   `\"grading_policy\"` with breakdowns of percentages\n",
    "\n",
    "In order to sufficiently answer the question about computer science courses, the LLM didn‚Äôt need all of this context‚Äîit really just needed the course titles and descriptions. But by sending everything, we pay the price in multiple ways:\n",
    "\n",
    "1. Financial waste: LLMs have an associated per-token cost, and in this case, roughly 90% of those tokens were unnecessary.\n",
    "2. Latency: Processing 6,000 tokens takes significantly longer than processing 500.\n",
    "3. Distraction: When you flood the LLM with irrelevant data, it‚Äôs more likely to get confused or ‚Äúhallucinate‚Äù ‚Äî like trying to find one phone number by reading every book in the library. This has been proven through the research on context rot by Chroma, which we covered in the introduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf454bf-ffbf-4e10-8e09-e0ce1bc28c88",
   "metadata": {},
   "source": [
    "## Wrap Up üèÅ\n",
    "\n",
    "You've completed Stage 1 and run the RAG agent. While it works, you've also experienced firsthand the core challenge of context engineering: more context isn't always better.\n",
    "\n",
    "In this stage, you:\n",
    "\n",
    "- Gained familiarity with the basic RAG pipeline that retrieves and synthesizes course information\n",
    "- Observed information overload when sending full documents to the LLM\n",
    "- Measured the token cost of a naive \"retrieve everything\" approach\n",
    "\n",
    "The key insight from this baseline is that retrieving full documents is rarely the right strategy. By sending 6,000+ tokens for a simple query that likely required only 500, you witnessed both financial waste and potential distraction that can lead to hallucinations.\n",
    "\n",
    "In Stage 2, you'll address this issue through basic data engineering techniques on the context, including trimming unnecessary data, filtering to display only summaries initially, and formatting with clean Markdown instead of raw JSON. You'll see how strategic context curation can reduce token usage by 80% while maintaining‚Äîor even improving‚Äîanswer quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
