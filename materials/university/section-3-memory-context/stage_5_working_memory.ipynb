{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6896e296",
   "metadata": {},
   "source": [
    "# Stage 5: Working Memory for Multi-Turn Conversations\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In Stage 4, we built a ReAct agent that could dynamically reason about its context needs; thinking through what information it needed, using tools to retrieve it, and looping until it had enough context to answer confidently. At the end of Stage 4, we also demonstrated multi-turn conversations by manually managing a conversation history list:\n",
    "\n",
    "```python\n",
    "# Stage 4 - Multi-turn conversation approach\n",
    "conversation_history = []\n",
    "\n",
    "# Turn 1\n",
    "result1 = await app.ainvoke({\n",
    "    \"input\": \"Tell me about CS002\", \n",
    "    \"history\": conversation_history\n",
    "})\n",
    "conversation_history.append(f\"User: Tell me about CS002\")\n",
    "conversation_history.append(f\"Agent: {result1['final_response'][:200]}...\")\n",
    "\n",
    "# Turn 2 - with reference to previous turn\n",
    "result2 = await app.ainvoke({\n",
    "    \"input\": \"What are the prerequisites for that course?\",\n",
    "    \"history\": conversation_history  # Agent can now see Turn 1\n",
    "})\n",
    "```\n",
    "\n",
    "This was our first introduction to managing conversational context through \"memory\". When we manually passed the conversation history, the agent successfully resolved \"that course\" to CS002. The reasoning capabilities were there, but the memory infrastructure wasn't. We couldn't persist conversations across sessions, extract important facts for later recall, deduplicate repeated information, or manage memory as conversations grew longer.\n",
    "\n",
    "Consider this a different scenario showing the limitation:\n",
    "\n",
    "**Turn 1:**  \n",
    "üë§ \"What is CS002?\"  \n",
    "ü§ñ \"CS002 is Data Structures and Algorithms...\"\n",
    "\n",
    "**Turn 2:**\n",
    "üë§ \"What are the prerequisites for that course?\"  \n",
    "ü§ñ ‚ùå \"I'm not sure which course you are referring to\" *(Agen't doesn't understand what \"that course\" refers to)*\n",
    "\n",
    "Solving this type of limitation is the focus of Stage 5, where we will introduce memory and shift the focus from managing retrieved context to managing conversational context. To do this, we'll use a new tool: [Redis Agent Memory Server (RAMS)](https://github.com/redis/agent-memory-server). \n",
    "\n",
    "RAMS is a fast and flexible memory layer for AI agents. Adding it will complete our agent stack, transforming our agent from a powerful single-query reasoner into a true conversational partner with persistent context (memory) across sessions.\n",
    "\n",
    "Before we begin working with memory, let's examine the key concepts surrounding how RAMS manages memory and its role within our agent architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9345a080",
   "metadata": {},
   "source": [
    "## Memory Architecture\n",
    "\n",
    "### How RAMS Manages Memory: The Two-Tier Model\n",
    "\n",
    "At the heart of implementing the memory infrastructure of our agent is how RAMS manages conversational context. RAMS isn't just a key-value store for conversations‚Äîit implements a two-tier memory system:\n",
    "\n",
    "1. Working Memory (Session-Scoped)  \n",
    "\n",
    "Working memory represents recent conversation history for the current session. It captures the immediate context of the ongoing conversation. Each session maintains its own working memory, perfect for managing multiple concurrent conversations per user.\n",
    "\n",
    "2. Long-Term Memory (Cross-Session)  \n",
    "\n",
    "On the other hand, long-term memory represents persistent facts extracted from all conversations over time. It stores important information that transcends individual conversations. Facts are automatically extracted, deduplicated, and compressed for efficient retrieval.\n",
    "\n",
    "Here's a quick comparison of the two memory types:\n",
    "\n",
    "| Working Memory | Long-Term Memory |\n",
    "|---|---|\n",
    "| Session-scoped | User-scoped OR Application-scoped |\n",
    "| Current conversation | Important facts, rules, knowledge |\n",
    "| Persists for session | Persists across sessions |\n",
    "| Full message history | Extracted knowledge (user + domain) |\n",
    "| Loaded/saved each turn | Searched when needed |\n",
    "| Challenge: Context window limits | Challenge: Storage growth |\n",
    "\n",
    "In this stage, we'll focus exclusively on working memory. The agent will load and saves conversation history, while RAMS will extract facts to long-term storage invisibly. In Stage 6 will add explicit tools (`search_memories`, `store_memory`) for the agent to deliberately query and manipulate long-term memory, but for now, we won't need to worry about it.\n",
    "\n",
    "Once implemented, our Stage 5 architecture wraps the ReAct agent with a memory layer:\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    Q[Query] --> LM[Load Working Memory]\n",
    "    LM --> IC[Classify Intent]\n",
    "    IC -->|GREETING| HG[Handle Greeting]\n",
    "    IC -->|Other| RA[ReAct Agent]\n",
    "\n",
    "    subgraph ReAct Loop\n",
    "        RA --> T1[üí≠ Thought: Analyze + use history]\n",
    "        T1 --> A1[üîß Action: search_courses]\n",
    "        A1 --> O1[üëÅÔ∏è Observation: Results]\n",
    "        O1 --> T2[üí≠ Thought: Evaluate]\n",
    "        T2 --> |Need more| A1\n",
    "        T2 --> |Done| F[‚úÖ FINISH]\n",
    "    end\n",
    "\n",
    "    F --> SM[Save Working Memory]\n",
    "    HG --> SM\n",
    "    SM --> END[Response + Reasoning Trace]\n",
    "\n",
    "    subgraph Memory Layer\n",
    "        LM -.->|Read| RAMS[(Redis Agent Memory Server)]\n",
    "        SM -.->|Write| RAMS\n",
    "    end\n",
    "```\n",
    "\n",
    "### The Working Memory Lifecycle\n",
    "\n",
    "With RAMS managing the storage and organization of memories, our agent needs to orchestrate when and how to interact with it. The working memory lifecycle in Stage 5 will follow three steps on every turn:\n",
    "\n",
    "1. Load Working Memory: At the start of each turn, the agent will load the conversation history from RAMS. This provides the immediate context for the current conversation.\n",
    "\n",
    "2. Process: The agent will use the conversation history as input to its reasoning process. This allows it to understand references like \"that course\" in context.\n",
    "\n",
    "3. Save Working Memory: After generating a response, the agent will save the new conversation turn back to RAMS. This updates the working memory for future turns.\n",
    "\n",
    "This pattern repeats on every turn:\n",
    "- **Before reasoning**: Load history to provide context\n",
    "- **During reasoning**: ReAct agent uses history (\"that course\" ‚Üí CS002)\n",
    "- **After reasoning**: Save the new exchange for future turns\n",
    "\n",
    "### Implementation Overview\n",
    "\n",
    "In this notebook, you'll implement the core memory infrastructure for working memory by building the following nodes: \n",
    "\n",
    "1. **`load_working_memory_node`**  \n",
    "A LangGraph node that retrieves conversation history from RAMS at the start of each turn and adds it to the agent's state.\n",
    "\n",
    "2. **`save_working_memory_node`**  \n",
    "A LangGraph node that stores the new conversation turn (user query + agent response) back to RAMS after reasoning completes.\n",
    "\n",
    "Before diving into implementation, let's review the supporting code that has already been implemented for you in the production agent. You'll focus on implementing the memory nodes, while these components handle configuration, state management, and workflow orchestration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b11c8f",
   "metadata": {},
   "source": [
    "## Implementing Working Memory\n",
    "\n",
    "### Supporting Infrastructure\n",
    "\n",
    "There are a few pieces of supporting infrastructure in the agent code that you should be aware of.\n",
    "\n",
    "For one, we abstract away configuration details for the Redis Agent Memory Server located in the [nodes.py](../progressive_agents/stage5_working_memory/agent/nodes.py#L55-L62) file.\n",
    "\n",
    "```python\n",
    "def get_memory_client() -> MemoryAPIClient:\n",
    "    \"\"\"Get the configured Agent Memory Server client.\"\"\"\n",
    "    config = MemoryClientConfig(\n",
    "        base_url=os.getenv(\"AGENT_MEMORY_URL\", \"http://localhost:8088\"),\n",
    "        default_namespace=\"course_qa_agent\",\n",
    "    )\n",
    "    return MemoryAPIClient(config=config)\n",
    "```\n",
    "\n",
    "Additionally, the workflow for the agent is now modified to use a TypedDict to track all state fields across nodes in the [state.py](../progressive_agents/stage5_working_memory/agent/state.py#L28-L80) file. \n",
    "\n",
    "For implementing memory, you'll notice there are `session_id`, `student_id`, and `conversation_history` fields: \n",
    "\n",
    "```python\n",
    "class WorkflowState(TypedDict):\n",
    "    # Core fields you'll work with:\n",
    "    session_id: str                          # Session identifier\n",
    "    student_id: str                          # User identifier  \n",
    "    conversation_history: List[Dict[str, str]]  # Previous messages\n",
    "    user_query: str                          # Current question\n",
    "    agent_response: str                      # Current answer\n",
    "    \n",
    "    # Plus many other fields for intent classification,\n",
    "    # entity extraction, caching, metrics, etc.\n",
    "```\n",
    "\n",
    "This state flows through every node in the workflow, with each node reading and updating relevant fields.\n",
    "\n",
    "Lastly, the complete workflow orchestrates multiple nodes with conditional routing in the [workflow.py#L67-L100](../progressive_agents/stage5_working_memory/agent/workflow.py#L67-L100) file. You'll implement the `load_working_memory_node` and `save_working_memory_node` functions that plug into this graph as noted below.\n",
    "\n",
    "```python\n",
    "workflow = StateGraph(WorkflowState)\n",
    "\n",
    "# Add all nodes\n",
    "workflow.add_node(\"load_memory\", load_working_memory_node)\n",
    "workflow.add_node(\"classify_intent\", classify_intent_node)\n",
    "workflow.add_node(\"react_agent\", react_agent_node)\n",
    "workflow.add_node(\"save_memory\", save_working_memory_node)\n",
    "\n",
    "# Define flow with conditional routing\n",
    "workflow.set_entry_point(\"load_memory\")\n",
    "workflow.add_edge(\"load_memory\", \"classify_intent\")\n",
    "workflow.add_conditional_edges(\"classify_intent\", route_after_intent, {...})\n",
    "workflow.add_edge(\"react_agent\", \"save_memory\")\n",
    "workflow.add_edge(\"save_memory\", END)\n",
    "```\n",
    "\n",
    "Now, let's get started building them out for the agent. Run the code block below to set up the agent code for this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980345f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "project_root = Path(\"..\").resolve()\n",
    "\n",
    "stage5_path = project_root / \"progressive_agents\" / \"stage5_working_memory\"\n",
    "src_path = project_root / \"src\"\n",
    "\n",
    "load_dotenv(project_root / \".env\")\n",
    "\n",
    "sys.path.insert(0, str(src_path))\n",
    "sys.path.insert(0, str(stage5_path))\n",
    "\n",
    "from agent import setup_agent, create_workflow, WorkflowState, get_memory_client, MemoryMessage, WorkingMemory\n",
    "\n",
    "print(\"Initializing Stage 5 Agent...\")\n",
    "course_manager, _ = await setup_agent(auto_load_courses=True)\n",
    "workflow = create_workflow(course_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c1f30f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Part 1: Implementing loading the working memory\n",
    "\n",
    "The first step in our memory lifecycle is *loading* conversation history from Agent Memory Server. This happens before the agent begins reasoning, providing it with context from previous turns in the same session.\n",
    "\n",
    "The `load_working_memory_node` is a LangGraph node that:\n",
    "\n",
    "1. Connects to Agent Memory Server using the session ID\n",
    "2. Retrieves all previous messages from this session's working memory\n",
    "3. Adds those messages to the agent's state as `conversation_history`\n",
    "4. Returns the updated state with conversation context\n",
    "\n",
    "This node transforms our agent from stateless (starting fresh every time) to stateful (aware of previous turns).\n",
    "\n",
    "### üìå Task: Implement `load_working_memory_node`\n",
    "\n",
    "Your task is to implement the function that loads working memory from the Agent Memory Server and populates the state's conversation history.\n",
    "\n",
    "The function receives a `state` dictionary containing `session_id` and `student_id`. It will then use the Agent Memory Server client to retrieve the working memory, convert the messages to a format the agent can use, and add them to the state. \n",
    "\n",
    "You'll use `get_or_create_working_memory()` method to retrieve the session's conversation history.\n",
    "\n",
    "<details>\n",
    "<summary>üõ†Ô∏è Show Implementation Details</summary>\n",
    "<br>\n",
    "\n",
    "**Step 1: Retrieve Working Memory**\n",
    "\n",
    "Call the async method `.get_or_create_working_memory()` on the memory client with three parameters:\n",
    "- `session_id`: from `state[\"session_id\"]`\n",
    "- `user_id`: from `state[\"student_id\"]`  \n",
    "- `model_name`: use `\"gpt-4o-mini\"`\n",
    "\n",
    "This returns a tuple: `(metadata, working_memory)`. You only need the `working_memory` object (use `_` for the metadata you don't need).\n",
    "\n",
    "**Step 2: Extract and Format Messages**\n",
    "\n",
    "If the `working_memory` exists and has messages (`working_memory.messages`), do the following:\n",
    "1. Create an empty list called `conversation_history`\n",
    "2. Iterate through `working_memory.messages`\n",
    "3. For each message, append a dictionary with `{\"role\": msg.role, \"content\": msg.content}` to the list\n",
    "4. Set `state[\"conversation_history\"]` to this list\n",
    "\n",
    "If no working memory exists, then set the conversation history in the state to an empty list `[]`\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088baaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required types and functions for memory node implementation\n",
    "from agent import WorkflowState, get_memory_client\n",
    "\n",
    "# from agent_memory_client import MemoryMessage, WorkingMemory\n",
    "\n",
    "\n",
    "async def load_working_memory_node(state: WorkflowState) -> WorkflowState:\n",
    "    \"\"\"\n",
    "    Load working memory from Agent Memory Server and populate conversation history.\n",
    "    \n",
    "    Args:\n",
    "        state: Current workflow state with session_id and student_id\n",
    "        \n",
    "    Returns:\n",
    "        Updated state with conversation_history from working memory\n",
    "    \"\"\"\n",
    "    session_id = state[\"session_id\"]\n",
    "    student_id = state[\"student_id\"]\n",
    "    \n",
    "    try:\n",
    "        # Get the memory client\n",
    "        memory_client = get_memory_client()\n",
    "        \n",
    "        # TODO - Step 1: Retrieve working memory for this session\n",
    "        \n",
    "        \n",
    "        # TODO - Step 2: Extract and format messages\n",
    "        if working_memory and working_memory.messages:\n",
    "            \n",
    "            print(f\"‚úÖ Loaded {len(conversation_history)} messages from working memory\")\n",
    "        else:\n",
    "            \n",
    "            print(\"‚ÑπÔ∏è No previous conversation history found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading working memory: {e}\")\n",
    "        state[\"conversation_history\"] = []\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"‚úÖ load_working_memory_node defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611c5c43",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üóùÔ∏è Solution code</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "async def load_working_memory_node(state: WorkflowState) -> WorkflowState:\n",
    "    \"\"\"\n",
    "    Load working memory from Agent Memory Server and populate conversation history.\n",
    "    \n",
    "    Args:\n",
    "        state: Current workflow state with session_id and student_id\n",
    "        \n",
    "    Returns:\n",
    "        Updated state with conversation_history from working memory\n",
    "    \"\"\"\n",
    "    session_id = state[\"session_id\"]\n",
    "    student_id = state[\"student_id\"]\n",
    "    \n",
    "    try:\n",
    "        # Get the memory client\n",
    "        memory_client = get_memory_client()\n",
    "        \n",
    "        # Retrieve working memory for this session\n",
    "        _, working_memory = await memory_client.get_or_create_working_memory(\n",
    "            session_id=session_id,\n",
    "            user_id=student_id,\n",
    "            model_name=\"gpt-4o-mini\"\n",
    "        )\n",
    "        \n",
    "        # Extract and format messages\n",
    "        if working_memory and working_memory.messages:\n",
    "            conversation_history = [\n",
    "                {\"role\": msg.role, \"content\": msg.content}\n",
    "                for msg in working_memory.messages\n",
    "            ]\n",
    "            state[\"conversation_history\"] = conversation_history\n",
    "            print(f\"‚úÖ Loaded {len(conversation_history)} messages from working memory\")\n",
    "        else:\n",
    "            state[\"conversation_history\"] = []\n",
    "            print(\"‚ÑπÔ∏è No previous conversation history found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading working memory: {e}\")\n",
    "        state[\"conversation_history\"] = []\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"‚úÖ load_working_memory_node defined.\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca7f56f",
   "metadata": {},
   "source": [
    "### Test the Load Node\n",
    "\n",
    "Let's verify our load node works correctly by testing both empty sessions and sessions with existing history. The test file handles setting up test data internally. Run the following code block to start the test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca354fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive load tests (empty session + session with history)\n",
    "from test_load_memory import run_tests\n",
    "\n",
    "await run_tests(load_working_memory_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cfee77",
   "metadata": {},
   "source": [
    "## Part 2: Implementing the Save Working Memory Node\n",
    "\n",
    "Now that we can load conversation history, we need a way to *save* new conversation turns back to the Agent Memory Server. This happens after the agent completes its response, storing both the user's question and the agent's answer.\n",
    "\n",
    "### Understanding the Save Node\n",
    "\n",
    "The `save_working_memory_node` is a LangGraph node that:\n",
    "\n",
    "1. Collects the complete conversation history (previous turns + current turn)\n",
    "2. Converts messages to Agent Memory Server's format\n",
    "3. Saves the updated working memory to the session\n",
    "4. Triggers automatic fact extraction (Agent Memory Server analyzes the conversation and extracts important facts to long-term memory)\n",
    "\n",
    "This node ensures that each turn is persisted, so the next turn can retrieve it via the load node.\n",
    "\n",
    "#### üìå Task: Implement `save_working_memory_node`\n",
    "\n",
    "Your task is to implement the function that saves the current conversation turn to Agent Memory Server's working memory.\n",
    "\n",
    "The function receives a `state` dictionary with `conversation_history` (previous turns), `user_query` (current question), and `agent_response` (current answer). \n",
    "\n",
    "It should combine all messages and save them to Agent Memory Server. You'll use `MemoryMessage` and `WorkingMemory` classes to format the data, then use the `.put_working_memory()` method to save it.\n",
    "\n",
    "<details>\n",
    "<summary>üõ†Ô∏è Show Implementation Details</summary>\n",
    "<br>\n",
    "\n",
    "**Step 1: Build Complete Message History**\n",
    "\n",
    "Create a list called `all_messages` that combines:\n",
    "- All messages from `state.get(\"conversation_history\", [])` (previous turns)\n",
    "- The current user message: `{\"role\": \"user\", \"content\": state[\"user_query\"]}`\n",
    "- The current agent message: `{\"role\": \"assistant\", \"content\": state[\"agent_response\"]}`\n",
    "\n",
    "**Step 2: Convert to Agent Memory Server Format**\n",
    "\n",
    "Create a list called `memory_messages` by converting each message dictionary to Agent Memory Server's `MemoryMessage` object:\n",
    "\n",
    "```python\n",
    "memory_messages = [\n",
    "    MemoryMessage(role=msg[\"role\"], content=msg[\"content\"])\n",
    "    for msg in all_messages\n",
    "]\n",
    "```\n",
    "\n",
    "**Step 3: Create Working Memory Object**\n",
    "\n",
    "Create a `WorkingMemory` object with:\n",
    "- `session_id`: from `state[\"session_id\"]`\n",
    "- `user_id`: from `state[\"student_id\"]`\n",
    "- `messages`: the `memory_messages` list\n",
    "- `memories`: empty list `[]`\n",
    "- `data`: empty dictionary `{}`\n",
    "\n",
    "**Step 4: Save to Agent Memory Server**\n",
    "\n",
    "Call the async method `.put_working_memory()` on the memory client with four parameters:\n",
    "- `session_id`: the `session_id` variable\n",
    "- `memory`: the `working_memory` object you created\n",
    "- `user_id`: the `student_id` variable\n",
    "- `model_name`: use `\"gpt-4o-mini\"`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e98629",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def save_working_memory_node(state: WorkflowState) -> WorkflowState:\n",
    "    \"\"\"\n",
    "    Save current conversation turn to Agent Memory Server.\n",
    "    \n",
    "    Args:\n",
    "        state: Current workflow state with conversation history and current turn\n",
    "        \n",
    "    Returns:\n",
    "        Unchanged state (saving is a side effect)\n",
    "    \"\"\"\n",
    "    session_id = state[\"session_id\"]\n",
    "    student_id = state[\"student_id\"]\n",
    "    \n",
    "    try:\n",
    "        # Get the memory client\n",
    "        memory_client = get_memory_client()\n",
    "\n",
    "        # TODO - Step 1: Build complete message history (previous + current turn)\n",
    "\n",
    "        # TODO - Step 2: Convert to Agent Memory Server format\n",
    "        memory_messages = []\n",
    "        \n",
    "        # TODO - Step 3: Create WorkingMemory object\n",
    "        \n",
    "        # TODO - Step 4: Save to Agent Memory Server\n",
    "\n",
    "        print(f\"‚úÖ Saved {len(memory_messages)} messages to working memory\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error saving working memory: {e}\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"‚úÖ save_working_memory_node defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6291a6f",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üóùÔ∏è Solution code</summary>\n",
    "<br> \n",
    "    \n",
    "```python\n",
    "\n",
    "async def save_working_memory_node(state: WorkflowState) -> WorkflowState:\n",
    "    \"\"\"\n",
    "    Save current conversation turn to Agent Memory Server.\n",
    "    \n",
    "    Args:\n",
    "        state: Current workflow state with conversation history and current turn\n",
    "        \n",
    "    Returns:\n",
    "        Unchanged state (saving is a side effect)\n",
    "    \"\"\"\n",
    "    session_id = state[\"session_id\"]\n",
    "    student_id = state[\"student_id\"]\n",
    "    \n",
    "    try:\n",
    "        # Get the memory client\n",
    "        memory_client = get_memory_client()\n",
    "        \n",
    "        # Build complete message history (previous + current turn)\n",
    "        all_messages = state.get(\"conversation_history\", []).copy()\n",
    "        all_messages.append({\"role\": \"user\", \"content\": state[\"user_query\"]})\n",
    "        all_messages.append({\"role\": \"assistant\", \"content\": state[\"agent_response\"]})\n",
    "        \n",
    "        # Convert to Agent Memory Server format\n",
    "        memory_messages = [\n",
    "            MemoryMessage(role=msg[\"role\"], content=msg[\"content\"])\n",
    "            for msg in all_messages\n",
    "        ]\n",
    "        \n",
    "        # Create WorkingMemory object\n",
    "        working_memory = WorkingMemory(\n",
    "            session_id=session_id,\n",
    "            user_id=student_id,\n",
    "            messages=memory_messages,\n",
    "            memories=[],\n",
    "            data={},\n",
    "        )\n",
    "        \n",
    "        # Save to Agent Memory Server\n",
    "        await memory_client.put_working_memory(\n",
    "            session_id=session_id,\n",
    "            memory=working_memory,\n",
    "            user_id=student_id,\n",
    "            model_name=\"gpt-4o-mini\",\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Saved {len(memory_messages)} messages to working memory\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error saving working memory: {e}\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"‚úÖ save_working_memory_node defined.\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bc7faa",
   "metadata": {},
   "source": [
    "### Test the Save Node\n",
    "\n",
    "Let's verify our save node works correctly by testing both new sessions and sessions with existing history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52a4aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive save tests (new session + session with history)\n",
    "from test_save_memory import run_tests\n",
    "\n",
    "await run_tests(save_working_memory_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18d3753",
   "metadata": {},
   "source": [
    "## Part 3: Testing Multi-Turn Conversations\n",
    "\n",
    "You've now implemented the core memory nodes that enable conversation history! While you wrote the `load_working_memory_node` and `save_working_memory_node` functions above, we won't integrate them into the workflow in this notebook. Instead, we'll use the production implementations from [nodes.py](../progressive_agents/stage5_working_memory/agent/nodes.py) which include the same logic you just built, plus:\n",
    "\n",
    "- Additional error handling and retry logic\n",
    "- Detailed logging for debugging\n",
    "- Memory client connection pooling\n",
    "- Metrics tracking for monitoring\n",
    "\n",
    "The production workflow in [workflow.py](../progressive_agents/stage5_working_memory/agent/workflow.py) already has these nodes integrated into the full pipeline: `Load Memory ‚Üí Classify Intent ‚Üí ReAct Agent ‚Üí Save Memory`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5665a8d8",
   "metadata": {},
   "source": [
    "Now, let's run [test_react_multi_turn.py](../progressive_agents/stage5_working_memory/test_react_multi_turn.py) which runs 4 comprehensive multi-turn conversation scenarios:\n",
    "\n",
    "1. Pronoun Resolution Test\n",
    "   - \"What is CS002?\" ‚Üí \"What are the prerequisites for it?\" ‚Üí \"Tell me more about the syllabus\"\n",
    "   - Demonstrates how the agent resolves \"it\" to CS002 using conversation history\n",
    "\n",
    "2. Follow-up Questions Test\n",
    "   - \"Tell me about machine learning courses\" ‚Üí \"Which one is best for beginners?\" ‚Üí \"What are the prerequisites for that course?\"\n",
    "   - Shows context retention across increasingly specific follow-ups\n",
    "\n",
    "3. Comparison Across Turns Test\n",
    "   - \"What is CS001?\" ‚Üí \"What is CS002?\" ‚Üí \"Which one should I take first?\"\n",
    "   - Tests the agent's ability to compare courses mentioned in separate turns\n",
    "\n",
    "4. Context Accumulation Test\n",
    "   - 4-turn conversation building progressively specific queries about computer vision courses\n",
    "   - Demonstrates how context accumulates and informs later responses\n",
    "\n",
    "Run the code block below to begin the test. Note that in addition to running the tests above, the `run_tests()` function will return some session_ids that we will use in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caf1f33",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import and run all multi-turn conversation tests\n",
    "from test_react_multi_turn import run_tests\n",
    "\n",
    "# Run test and store session IDs for later use in compression analysis\n",
    "\n",
    "test_session_ids = await run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c809856",
   "metadata": {},
   "source": [
    "## Preview: Automatic Knowledge Extraction\n",
    "\n",
    "### From Conversations to Persistent Knowledge\n",
    "\n",
    "Working memory is important because it captures the current session, but what happens to valuable information when the session ends? If a student spends 20 minutes discussing their interest in machine learning courses, that knowledge shouldn't disappear when they close their browser.\n",
    "\n",
    "This is where RAMS's two-tier memory system shines. Every time you call `put_working_memory()` to save a conversation turn, RAMS automatically performs several intelligent operations behind the scenes:\n",
    "\n",
    "1. **Analyzes the Conversation Turn**  \n",
    "RAMS uses an LLM to examine the user query and agent response, identifying important information worth preserving long-term.\n",
    "\n",
    "2. **Extracts Key Facts**  \n",
    "Instead of storing verbatim conversation history, RAMS extracts compressed facts:\n",
    "   - \"Student is interested in machine learning courses\"\n",
    "   - \"Student asked about CS004 prerequisites\"\n",
    "   - \"Student prefers beginner-level content\"\n",
    "\n",
    "3. **Stores in Long-Term Memory**  \n",
    "These facts are saved to long-term memory with vector embeddings for semantic search and graph relationships for structured queries. They persist across sessions and are available to all future conversations for that user.\n",
    "\n",
    "4. **Deduplicates Information**  \n",
    "If a student asks \"What is CS002?\" multiple times across turns, RAMS recognizes the redundancy and maintains a single fact rather than duplicating information.\n",
    "\n",
    "This entire pipeline happens automatically and invisibly in Stage 5. You don't need to call any extraction APIs or manage the transition from working memory to long-term memory‚ÄîRAMS handles it transparently when you save working memory.\n",
    "\n",
    "> **Note:** RAMS supports [multiple memory extraction strategies](https://redis.github.io/agent-memory-server/memory-extraction-strategies/) that you can configure to customize how facts are identified and stored (such as entity extraction, topic modeling, or custom extraction prompts). In this course, we use the default extraction strategy and don't explicitly configure it in our RAMS setup which allows RAMS to intelligently handle fact extraction out of the box.\n",
    "\n",
    "The automatic fact extraction also creates a powerful compression mechanism:\n",
    "\n",
    "```\n",
    "Working Memory:                   Long-Term Memory:\n",
    "Turn 1: \"What is CS004?\"         ‚Üí Fact: \"Student interested in CS004\"\n",
    "  Response: 300 tokens\n",
    "Turn 2: \"Prerequisites?\"         ‚Üí Fact: \"Student asked about prerequisites\"\n",
    "  Response: 200 tokens\n",
    "Turn 3: \"More details?\"          ‚Üí Merged with: \"Student needs CS004 info\"\n",
    "  Response: 250 tokens\n",
    "\n",
    "Total: ~750 tokens               Total: ~50 tokens (15x compression)\n",
    "```\n",
    "\n",
    "Instead of loading 750 tokens of conversation history on future turns, the agent could search long-term memory and retrieve the compressed fact: \"Student is interested in CS004, asked about prerequisites and details.\" This achieves typical 10:1 to 20:1 compression ratios, dramatically extending the effective conversation length before hitting context limits.\n",
    "\n",
    "### Seeing Long-Term Memory in Action\n",
    "\n",
    "Let's inspect what RAMS has automatically extracted from our test conversations without us explicitly doing anything. After running the multi-turn tests above, RAMS has been analyzing and extracting facts behind the scenes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e16736d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Query long-term memories automatically extracted by RAMS\n",
    "from query_long_term_memory import query_extracted_memories, analyze_compression\n",
    "\n",
    "# Query memories for the test user using the AMS API\n",
    "# Note: query_extracted_memories is a wrapper and is not part of the AMS API/Methods. Check out the query_long_term_memory.py to see the full implementation.\n",
    "memories = await query_extracted_memories(\n",
    "    student_id=\"test_user\",\n",
    "    search_query=\"courses discussed\",\n",
    "    limit=20\n",
    ")\n",
    "\n",
    "# Compare token size: working memory vs long-term memory\n",
    "# Use the session IDs we stored from running the tests above\n",
    "if 'test_session_ids' in dir() and len(test_session_ids) > 0:\n",
    "    print(f\"\\nüîç Using {len(test_session_ids)} sessions from test run:\")\n",
    "    for session_id in test_session_ids:\n",
    "        print(f\"  ‚Ä¢ {session_id}\")\n",
    "    \n",
    "    print(\"\\nüìä Running compression analysis...\\n\")\n",
    "    await analyze_compression(test_session_ids, student_id=\"test_user\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No test_session_ids found. Run the multi-turn tests first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4f3885-17cd-439f-a0a7-661a455d9e05",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Open this dropdown after running the code block to review the compression analysis</summary>\n",
    "<br>\n",
    "    \n",
    "The compression analysis above demonstrates a critical principle of managing conversational context: quality over quantity. By extracting and compressing facts from verbose conversation history, RAMS enables the agent to maintain context over much longer interactions without the risk of token limits. In this case, a compression ratio of ~1-2.5x is achieved compared to storing the full message history.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9d1ef0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Wrap Up üèÅ\n",
    "\n",
    "Excellent work! You've completed Stage 5 and transformed your agent from a single-turn reasoner into a conversational partner with persistent memory.\n",
    "\n",
    "In this stage, you learned how to:\n",
    "\n",
    "- Implement the `load_working_memory` node that retrieves conversation history from RAMS at the start of each turn\n",
    "- Implement the `save_working_memory` node that persists conversation turns back to RAMS after reasoning completes\n",
    "- Orchestrate the Load ‚Üí Process ‚Üí Save lifecycle that enables multi-turn conversations with context awareness\n",
    "\n",
    "You also got a preview of how RAMS automatically extracts and compresses facts from working memory to long-term memory behind the scenes.\n",
    "\n",
    "In Stage 5, this automatic extraction happened invisibly‚Äîuseful for managing context window limits, but not directly accessible to your agent's reasoning process. Stage 6 changes that by giving your agent *explicit control* over long-term memory via LangChain tools.\n",
    "\n",
    "You'll transform the agent from passively benefiting from automatic extraction to actively managing its own memory‚Äîdeciding what to remember, what to recall, and when to search its accumulated knowledge.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
