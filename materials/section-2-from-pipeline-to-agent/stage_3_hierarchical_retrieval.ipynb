{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5256b61-8ed8-4170-bcb9-e1c5b08c78d8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Stage 3: Hierarchical Retrieval & Intent-Driven Context\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Recall that at the beginning of this course, we defined context engineering as the practice of deliberately shaping what information reaches the LLM, and in what form. It's not just about *what* we retrieve â€” it's also about *how we structure it*, *when we include it*, and *how much detail we provide*.\n",
    "\n",
    "In our stage 1 naive RAG, we saw that if we include *everything* in the context (the entire course catalog), we overwhelm the LLM, waste tokens, and, based on research from top AI companies, would eventually create issues like context rot.\n",
    "\n",
    "In stage 2, we tackled how to structure the context. We explored how to start to shape data to be both LLM-friendly and task-appropriate. We built two representations of the same data:\n",
    "- `transform_course_to_text` gave us a full, human-readable format (structured for comprehension)\n",
    "- `optimize_course_text` gave us a compressed, essentials-only version (optimized for token efficiency)\n",
    "\n",
    "This was our way of presenting the same piece of information at different granularities, and *how* we format it matters as much as what we include.\n",
    "\n",
    "But this implementation was simple because it had a critical limitation: the context was static. We chose one representation strategy (full or optimized) at the start, and every query â€” whether *\"What machine learning courses exist?\"* or *\"What is the week 1 assignment for CS101?\"* â€” received the same treatment. The pipeline didn't adapt to the question.\n",
    "\n",
    "In this stage, we'll add intelligence: instead of choosing one strategy for all queries, we'll build a system that analyzes the user's intent and dynamically adjusts the context's structure and depth in response.\n",
    "\n",
    "Here's what we'll build:\n",
    "\n",
    "1. **Hierarchical Data Models (Structure)**  \n",
    "   Instead of choosing between \"full\" or \"optimized,\" we'll formalize two distinct *views* of the data:\n",
    "   - Summary View: Lightweight, ~50-100 tokens per item. Perfect for browsing.\n",
    "   - Details View: Comprehensive, ~500-1000 tokens per item. Contains the full syllabus, assignments, and learning objectives.\n",
    "   \n",
    "   This is a layered context design, providing the system with multiple lenses through which to view the same underlying data.\n",
    "\n",
    "2. **Context Assembly with Progressive Disclosure (Presentation)**  \n",
    "   We'll use research-backed techniques to structure context for maximum LLM comprehension:\n",
    "   - Place summaries at the beginning (high-level map)\n",
    "   - Place details at the end (specifics are \"fresh\" when generating)\n",
    "   \n",
    "   This combats the [\"Lost in the Middle\"](https://arxiv.org/abs/2307.03172) phenomenon and is positional context optimization.\n",
    "\n",
    "3. **Intent Classification (Decision)**  \n",
    "   Not all questions require the same information depth. We'll teach the system to classify user queries by information need:\n",
    "   - *\"What courses are there?\"* â†’ Need: breadth (summaries only)\n",
    "   - *\"What will I learn in CS101?\"* â†’ Need: depth (full details)\n",
    "   \n",
    "   This will serve as a query-aware context selection strategy where the context we provide adapts to the question.\n",
    "\n",
    "4. **Quality Evaluation (Validation)**  \n",
    "   Before sending context to the LLM, we'll validate: *\"Is this information actually sufficient to answer the question?\"*  \n",
    "   If not, we automatically retrieve again with a different strategy.\n",
    "   \n",
    "   This is feedback-driven context refinementâ€”the system learns from its mistakes in real-time.\n",
    "\n",
    "By the end of Stage 3, we'll have built a system that practices adaptive, validated context engineering:\n",
    "- It analyzes the question to determine the information depth needed\n",
    "- It assembles context hierarchically (breadth-first, then depth)\n",
    "- It validates quality before use\n",
    "- It automatically corrects insufficient context\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2564accd-46dc-41a9-a9ef-d49f3d75788d",
   "metadata": {},
   "source": [
    "## Setup and Agent Overview\n",
    "\n",
    "Let's set up our environment and import the stage 3 agent. Run the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb79738-5d0e-4da9-89fe-b7557337e554",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code sets up the notebook to be able to access the provided OpenAI API Key and access to the agent code\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "if \"OPENAI_API_BASE\" in os.environ:\n",
    "    os.environ[\"OPENAI_BASE_URL\"] = os.environ[\"OPENAI_API_BASE\"]\n",
    "\n",
    "project_root = Path(\"..\").resolve()\n",
    "\n",
    "stage3_path = project_root / \"progressive_agents\" / \"stage3_hierarchical_retrieval\"\n",
    "src_path = project_root / \"src\"\n",
    "\n",
    "sys.path.insert(0, str(src_path))\n",
    "sys.path.insert(0, str(stage3_path))\n",
    "\n",
    "from agent import setup_agent, create_workflow\n",
    "\n",
    "print(\"Initializing Stage 3 Agent...\")\n",
    "# This reuses the same Redis data from previous stages\n",
    "course_manager = await setup_agent(auto_load_courses=True)\n",
    "workflow = create_workflow(course_manager)\n",
    "\n",
    "print(\"âœ“ Agent is ready!\")\n",
    "print(\"âœ“ Course manager initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a330e38d-efaa-4b2b-b5f2-e8814cd7cce7",
   "metadata": {},
   "source": [
    "Before we start building, let's understand where these components reside in the actual Stage 3 agent workflow â€” and how it differs from what we built in Stage 2.\n",
    "\n",
    "In Stage 2, the workflow was linear:\n",
    "\n",
    "<br>\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    Start([User Query]) --> Research[\"Research Node<br/>Retrieve & assemble context\"]\n",
    "    Research --> Synthesize[\"Synthesize Node<br/>Generate answer\"]\n",
    "    Synthesize --> End([Final Response])\n",
    "\n",
    "    style Research fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px\n",
    "    style Synthesize fill:#fff9c4,stroke:#f57f17,stroke-width:2px\n",
    "```\n",
    "<br>\n",
    "\n",
    "\n",
    "In Stage 3, the workflow will have three main nodes. The agent node is where the magic happens:\n",
    "\n",
    "<br>\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    Start([User Query]) --> Classify[\"Classify Intent<br/>(Node)\"]\n",
    "\n",
    "    Classify -->|GREETING| Greeting[\"Handle Greeting<br/>(Node)\"]\n",
    "    Classify -->|GENERAL or DETAIL| Agent[\"Agent with Tool Calling<br/>(Node)\"]\n",
    "\n",
    "    Greeting --> End1([Return Greeting])\n",
    "\n",
    "    Agent -->|Use search_courses tool| Tool[\"search_courses Tool<br/>- Hierarchical models<br/>- Context assembler<br/>- Quality evaluator<br/>..\"]\n",
    "    \n",
    "    Tool -->|Good quality| Return[\"Return Answer\"]\n",
    "    Tool -->|Poor quality| Tool\n",
    "    \n",
    "    style Classify fill:#ffe0b2,stroke:#e65100,stroke-width:3px\n",
    "    style Greeting fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px\n",
    "    style Agent fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px\n",
    "    style Tool fill:#fff9c4,stroke:#f57f17,stroke-width:2px\n",
    "```\n",
    "\n",
    "The agent node is where we will call tools. The node will have access to call a `search_courses` tool, which internally will use:\n",
    "- The `CourseSummary` & `CourseDetails` models\n",
    "- The `ContextAssembler` for progressive disclosure\n",
    "- A `evaluate_context_quality` function to evaluate quality\n",
    "\n",
    "Now that you're familiar with the overall flow, let's explore the hierarchical models we'll be working with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2a7ee1-f4b5-416a-ba0f-2bffdbd72ed0",
   "metadata": {},
   "source": [
    "## Part 1: The Data Structure (Hierarchical Models)\n",
    "\n",
    "To start, we're going to formalize two distinct views of our data:\n",
    "1.  `CourseSummary`: A lightweight model (~50-100 tokens) for browsing and broad search.\n",
    "2.  `CourseDetails`: A comprehensive model (~500-1000 tokens) containing the full syllabus, assignments, and prerequisites.\n",
    "\n",
    "We'll keep the summary view fairly straightforward. It will simply be a flat list of fields, such as title, code, and instructor. \n",
    "\n",
    "On the other hand, the details view is a bit more complex. To represent a full course faithfully, we can't just use a giant string of text. We need structured data to allow for precise rendering and querying. A real university course has a syllabus composed of weekly plans, assignments with specific metadata (including due dates and points), and prerequisites.\n",
    "\n",
    "If we flattened all this into a single string, we'd lose the ability to format it dynamically or query specific parts (e.g., \"What is due in Week 3?\").\n",
    "\n",
    "To keep the agent organized, the data models are split into two files (which you can feel free to explore):\n",
    "*   `src/redis_context_course/models.py`: Contains the core domain entities and enums (like `DifficultyLevel`, `CourseFormat`) that are used everywhere.\n",
    "*   `src/redis_context_course/hierarchical_models.py`: Contains the specialized structures (like `WeekPlan`, `Assignment`) that are specifically designed for this hierarchical retrieval strategy.\n",
    "\n",
    "For example, here is what the `WeekPlan` specialized structure looks like:\n",
    "\n",
    "```python\n",
    "class WeekPlan(BaseModel):\n",
    "    \"\"\"Detailed plan for a single week of the course.\"\"\"\n",
    "    week_number: int\n",
    "    topic: str\n",
    "    subtopics: List[str] = Field(default_factory=list)\n",
    "    readings: List[str] = Field(default_factory=list)\n",
    "    assignments: List[str] = Field(default_factory=list)\n",
    "    learning_objectives: List[str] = Field(default_factory=list)\n",
    "```\n",
    "\n",
    "In addition to `WeekPlan`, there are also models for the following:\n",
    "*   `CourseSyllabus`: A collection of `WeekPlan` objects.\n",
    "*   `Assignment`: Structured details like `due_week`, `points`, and `type` (Exam vs Project).\n",
    "*   `Prerequisite`: Links to other course codes.\n",
    "\n",
    "These models act as the \"schema\" for the detailed view. The `CourseDetails` model will utilize them as fields (e.g., `syllabus: CourseSyllabus`), effectively serving as a container for this rich information.\n",
    "\n",
    "### The Two-Tier Storage Architecture\n",
    "\n",
    "Understanding how these models are stored in Redis is key to understanding why they're designed this way:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚      USER QUERY: \"machine learning courses\"             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                  â”‚\n",
    "                  â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  TIER 1: Vector Search on CourseSummary                 â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”‚\n",
    "â”‚  Storage: Redis Vector Index                            â”‚\n",
    "â”‚  Data: CourseSummary + Embeddings                       â”‚\n",
    "â”‚  Purpose: Fast semantic search across ALL courses       â”‚\n",
    "â”‚  Cost: ~50-100 tokens per course                        â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Query â†’ Embedding â†’ Vector Search â†’ Top 5 Course IDs   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                  â”‚\n",
    "                  â”‚ IDs: [\"CS401\", \"CS501\", \"CS601\"]\n",
    "                  â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  TIER 2: Direct Lookup of CourseDetails                 â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”‚\n",
    "â”‚  Storage: Redis Hash (key-value)                        â”‚\n",
    "â”‚  Data: CourseDetails (no embeddings needed)             â”‚\n",
    "â”‚  Purpose: Fetch full details by ID                      â”‚\n",
    "â”‚  Cost: ~500-1000 tokens per course                      â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Course IDs â†’ Direct Hash Lookup â†’ Full Details         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### The Two Model Implementations\n",
    "\n",
    "Below in the drop downs you'll find the complete model definitions. Explore the code for both to gain familiarity on how they work. \n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>ğŸ” Open to explore <code>CourseSummary</code></summary>\n",
    "\n",
    "<br>\n",
    "\n",
    "The lightweight \"card catalog\" view is used for browsing and initial search results. It contains only essential course information (~50-100 tokens per course).\n",
    "\n",
    "Note that it has a `generate_embedding_text()` method, which creates a searchable text representation that is converted to a vector embedding and stored alongside the course data.\n",
    "\n",
    "<br>\n",
    "\n",
    "```python\n",
    "\n",
    "class CourseSummary(BaseModel):\n",
    "    \"\"\"\n",
    "    Lightweight course overview for initial search.\n",
    "\n",
    "    This is the first tier in hierarchical retrieval.\n",
    "    Contains just enough information for users to decide\n",
    "    if they want more details.\n",
    "    \"\"\"\n",
    "\n",
    "    # Core identification\n",
    "    course_code: str\n",
    "    title: str\n",
    "\n",
    "    # Basic info\n",
    "    department: str\n",
    "    credits: int\n",
    "    difficulty_level: DifficultyLevel\n",
    "    format: CourseFormat\n",
    "    instructor: str\n",
    "\n",
    "    # Brief description (1-2 sentences)\n",
    "    short_description: str\n",
    "\n",
    "    # Prerequisites (just course codes for brevity)\n",
    "    prerequisite_codes: List[str] = Field(default_factory=list)\n",
    "\n",
    "    # Metadata for search\n",
    "    tags: List[str] = Field(default_factory=list)\n",
    "\n",
    "    # For vector search\n",
    "    embedding_text: Optional[str] = None\n",
    "\n",
    "    def generate_embedding_text(self) -> str:\n",
    "        \"\"\"Generate text for vector embedding.\"\"\"\n",
    "        parts = [\n",
    "            f\"{self.course_code}: {self.title}\",\n",
    "            f\"Department: {self.department}\",\n",
    "            f\"Level: {self.difficulty_level.value}\",\n",
    "            self.short_description,\n",
    "        ]\n",
    "        if self.tags:\n",
    "            parts.append(f\"Topics: {', '.join(self.tags)}\")\n",
    "\n",
    "        self.embedding_text = \" | \".join(parts)\n",
    "        return self.embedding_text\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ” Open to explore <code>CourseDetails</code></summary>\n",
    "\n",
    "<br>\n",
    "\n",
    "The comprehensive \"full book\" view with complete syllabus and assignments. Retrieved only for the most relevant courses (~500-1000 tokens per course). \n",
    "\n",
    "This model is stored as plain JSON in a Redis hash. It's never searched â€” it's fetched directly by course ID after the summary search finds relevant courses. Since we already know the exact ID we want, there's no need for vector search or embeddings on this data.\n",
    "\n",
    "<br>\n",
    "\n",
    "```python\n",
    "\n",
    "class CourseDetails(BaseModel):\n",
    "    \"\"\"\n",
    "    Full course details with syllabus and assignments.\n",
    "\n",
    "    This is the second tier in hierarchical retrieval.\n",
    "    Retrieved only for the most relevant courses after\n",
    "    initial summary search.\n",
    "    \"\"\"\n",
    "\n",
    "    # All fields from CourseSummary\n",
    "    course_code: str\n",
    "    title: str\n",
    "    department: str\n",
    "    credits: int\n",
    "    difficulty_level: DifficultyLevel\n",
    "    format: CourseFormat\n",
    "    instructor: str\n",
    "\n",
    "    # Full description (multiple paragraphs)\n",
    "    full_description: str\n",
    "\n",
    "    # Prerequisites (full details)\n",
    "    prerequisites: List[Prerequisite] = Field(default_factory=list)\n",
    "\n",
    "    # Learning outcomes\n",
    "    learning_objectives: List[str] = Field(default_factory=list)\n",
    "\n",
    "    # Detailed syllabus\n",
    "    syllabus: CourseSyllabus\n",
    "\n",
    "    # Assignments\n",
    "    assignments: List[Assignment] = Field(default_factory=list)\n",
    "\n",
    "    # Additional metadata\n",
    "    semester: Semester\n",
    "    year: int\n",
    "    max_enrollment: int\n",
    "    tags: List[str] = Field(default_factory=list)\n",
    "\n",
    "    def to_summary(self) -> CourseSummary:\n",
    "        \"\"\"Convert full details to summary view.\"\"\"\n",
    "        # Create short description from first 2 sentences of full description\n",
    "        sentences = self.full_description.split(\". \")\n",
    "        short_desc = \". \".join(sentences[:2])\n",
    "        if not short_desc.endswith(\".\"):\n",
    "            short_desc += \".\"\n",
    "\n",
    "        return CourseSummary(\n",
    "            course_code=self.course_code,\n",
    "            title=self.title,\n",
    "            department=self.department,\n",
    "            credits=self.credits,\n",
    "            difficulty_level=self.difficulty_level,\n",
    "            format=self.format,\n",
    "            instructor=self.instructor,\n",
    "            short_description=short_desc,\n",
    "            prerequisite_codes=[p.course_code for p in self.prerequisites],\n",
    "            tags=self.tags,\n",
    "        )\n",
    "```\n",
    "</details>\n",
    "\n",
    "Now that you've seen the model definitions, let's import them and verify that everything is working as expected. Run the cell below to import the models from the source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8c0b4a-c153-493a-ab83-d58be71e122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from redis_context_course.models import DifficultyLevel, CourseFormat\n",
    "from redis_context_course.hierarchical_models import (\n",
    "    CourseSummary,\n",
    "    CourseDetails,\n",
    "    Prerequisite, \n",
    "    WeekPlan, \n",
    "    CourseSyllabus, \n",
    "    Assignment, \n",
    "    AssignmentType\n",
    ")\n",
    "from pydantic import Field\n",
    "from typing import List\n",
    "\n",
    "print(\"âœ… Models imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fd65a6-cc7a-486c-8283-6a96f45a3993",
   "metadata": {},
   "source": [
    "With our hierarchical data models now imported, we're ready to move on to the next critical component: how we present this data to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cca3dd-459c-45e9-84c1-1e055a420d51",
   "metadata": {},
   "source": [
    "## Part 2: The Context Assembler (Progressive Disclosure)\n",
    "\n",
    "Having established our two-tier data structure, we now face a new challenge: how do we arrange this information to maximize the LLM's ability to understand and use it effectively? This is where progressive disclosure comes in.\n",
    "\n",
    "Progressive disclosure means showing only the information needed, when it's needed, in the optimal order. In traditional interfaces, this means starting with high-level categories, then allowing users to drill down into specifics.\n",
    "\n",
    "This matters because of how LLMs process context. Research shows they pay more attention to information at the beginning and end of their context window (the \"Lost in the Middle\" problem), while processing hierarchically structured information more effectively than flat lists. By placing summaries at the start, we create navigational landmarks. By placing details at the end, we leverage recency bias during generation. This is context engineering in action: deliberately shaping not just what information is included, but where it's positioned and how densely it's packed.\n",
    "\n",
    "For our course search system, we structure context in two layers. The overview layer places all `CourseSummary` objects at the start to create a high-level map, answering the question \"What options exist?\" The deep dive layer places full `CourseDetails` for the most relevant courses at the very endâ€”providing comprehensive information where the LLM's attention is naturally high.\n",
    "\n",
    "### ğŸ“Œ Task 1: Implement the Context Assembler\n",
    "\n",
    "Below you'll find a `ContextAssembler` class. Your task is to implement the two methods that decide *how* to arrange course information for the LLM.\n",
    "\n",
    "The parent class `HierarchicalContextAssembler` (located in `src/redis_context_course/hierarchical_context.py`) provides two helper methods you can call:\n",
    "- `self._format_summary(summary, index)` - Formats a single course summary with numbering\n",
    "- `self._format_details(details)` - Formats full course details including syllabus and assignments\n",
    "\n",
    "You'll need to complete the two main assembly strategies:\n",
    "\n",
    "1. **`assemble_summary_only_context(summaries, query)`**: For general queries like \"What CS courses exist?\"\n",
    "   - Return a string with: header, count, and all summaries formatted with `self._format_summary()`\n",
    "   - This is the \"breadth-first\" strategy (overview only)\n",
    "\n",
    "<details><summary>ğŸ› ï¸ Show Implementation Details</summary>\n",
    "<br>\n",
    "    \n",
    "Build a string by populating the `sections` list with the following:\n",
    " \n",
    "1. Add aheader using `sections.append()`: `f\"# Course Search Results for: {query}\\n\"`\n",
    "2. Add a count: `f\"Found {len(summaries)} relevant courses:\\n\"`\n",
    "3. Loop through summaries using `enumerate(summaries, 1)` to get index `i` starting from 1\n",
    "4. For each summary, call `self._format_summary(summary, i)` and append to sections\n",
    "5. Join all sections with `\"\\n\".join(sections)` and return the result\n",
    "\n",
    "</details>\n",
    "\n",
    "2. **`assemble_hierarchical_context(summaries, details, query)`**: For detailed queries like \"What will I learn in CS101?\"\n",
    "   - Return a string with: header, \"Overview\" section with all summaries, \"Detailed Information\" section with full details\n",
    "   - This is the \"progressive disclosure\" strategy (breadth first, then depth)\n",
    "\n",
    "<details><summary>ğŸ› ï¸ Show Implementation Details</summary>\n",
    "<br>    \n",
    "    \n",
    "Build a string by populating the `sections` list with:\n",
    " \n",
    "1. Add a header: `f\"# Course Search Results for: {query}\\n\"`\n",
    "2. Add an overview section:\n",
    "   - Header: `\"## Overview of All Matches\\n\"`\n",
    "   - Count: `f\"Found {len(summaries)} relevant courses:\\n\"`\n",
    "   - Loop through summaries with `enumerate(summaries, 1)` and append `self._format_summary(summary, i)`\n",
    "3. Add a details section (if details exist):\n",
    "   - Header: `f\"\\n## Detailed Information (Top {len(details)} Courses)\\n\"`\n",
    "   - Intro: `\"Full syllabi and assignments for the most relevant courses:\\n\"`\n",
    "   - Loop through details and append `self._format_details(detail)` for each\n",
    "4. Join all sections with `\"\\n\".join(sections)` and return the result\n",
    "</details>\n",
    "\n",
    "If you get stuck, reference the solution dropdown below the implementation cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11d51b6-04e7-46ee-944e-c0e26555be01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from redis_context_course.hierarchical_models import CourseSummary, CourseDetails\n",
    "from redis_context_course.hierarchical_context import HierarchicalContextAssembler\n",
    "from typing import List\n",
    "\n",
    "class ContextAssembler(HierarchicalContextAssembler):\n",
    "    \"\"\"\n",
    "    Assembles context using a progressive disclosure pattern.\n",
    "    \n",
    "    Note: The formatting helpers (_format_summary and _format_details) will be \n",
    "    provided by the production system when you connect to the agent in the Bonus section.\n",
    "    \"\"\"\n",
    "    \n",
    "    def assemble_summary_only_context(\n",
    "        self,\n",
    "        summaries: List[CourseSummary],\n",
    "        query: str,\n",
    "    ) -> str:\n",
    "        \"\"\"Assemble context with ONLY summaries (no details).\"\"\"\n",
    "        sections = []\n",
    "        \n",
    "        # TODO: Add header with query\n",
    "        \n",
    "        # TODO: Add count of summaries found\n",
    "        \n",
    "        # TODO: Loop through summaries with enumerate(summaries, 1)\n",
    "        \n",
    "        return \"\\n\".join(sections)\n",
    "    \n",
    "    def assemble_hierarchical_context(\n",
    "        self,\n",
    "        summaries: List[CourseSummary],\n",
    "        details: List[CourseDetails],\n",
    "        query: str,\n",
    "    ) -> str:\n",
    "        \"\"\"Assemble context with progressive disclosure (summaries + details).\"\"\"\n",
    "        sections = []\n",
    "        \n",
    "        # TODO: Add header with query\n",
    "        \n",
    "        # TODO: Add \"Overview of All Matches\" section\n",
    "\n",
    "        # TODO: Add \"Detailed Information\" section if details exist\n",
    "\n",
    "        \n",
    "        return \"\\n\".join(sections)\n",
    "\n",
    "# Create the assembler instance\n",
    "assembler = ContextAssembler()\n",
    "\n",
    "print(\"âœ… Context assembler created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693451d9-07d5-4581-a2ed-6baefb79c71b",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ğŸ—ï¸ Solution code</summary>\n",
    "    \n",
    "<br>\n",
    "    \n",
    "```python\n",
    "\n",
    "from redis_context_course.hierarchical_models import CourseSummary, CourseDetails\n",
    "from redis_context_course.hierarchical_context import HierarchicalContextAssembler\n",
    "from typing import List\n",
    "\n",
    "class ContextAssembler(HierarchicalContextAssembler):\n",
    "    \"\"\"\n",
    "    Assembles context using a progressive disclosure pattern.\n",
    "    \n",
    "    Note: The formatting helpers (_format_summary and _format_details) will be \n",
    "    provided by the production system when you connect to the agent in the Bonus section.\n",
    "    \"\"\"\n",
    "    \n",
    "    def assemble_summary_only_context(\n",
    "        self,\n",
    "        summaries: List[CourseSummary],\n",
    "        query: str,\n",
    "    ) -> str:\n",
    "        \"\"\"Assemble context with ONLY summaries (no details).\"\"\"\n",
    "        sections = []\n",
    "        \n",
    "        # Header\n",
    "        sections.append(f\"# Course Search Results for: {query}\\n\")\n",
    "        \n",
    "        # Summary count and list\n",
    "        sections.append(f\"Found {len(summaries)} relevant courses:\\n\")\n",
    "        \n",
    "        # Format each summary\n",
    "        for i, summary in enumerate(summaries, 1):\n",
    "            sections.append(self._format_summary(summary, i))\n",
    "        \n",
    "        return \"\\n\".join(sections)\n",
    "    \n",
    "    def assemble_hierarchical_context(\n",
    "        self,\n",
    "        summaries: List[CourseSummary],\n",
    "        details: List[CourseDetails],\n",
    "        query: str,\n",
    "    ) -> str:\n",
    "        \"\"\"Assemble context with progressive disclosure (summaries + details).\"\"\"\n",
    "        sections = []\n",
    "        \n",
    "        # Header\n",
    "        sections.append(f\"# Course Search Results for: {query}\\n\")\n",
    "        \n",
    "        # Section 1: Overview of ALL matches (breadth-first)\n",
    "        sections.append(\"## Overview of All Matches\\n\")\n",
    "        sections.append(f\"Found {len(summaries)} relevant courses:\\n\")\n",
    "        \n",
    "        for i, summary in enumerate(summaries, 1):\n",
    "            sections.append(self._format_summary(summary, i))\n",
    "        \n",
    "        # Section 2: Detailed Information for TOP matches (depth at end)\n",
    "        if details:\n",
    "            sections.append(f\"\\n## Detailed Information (Top {len(details)} Courses)\\n\")\n",
    "            sections.append(\"Full syllabi and assignments for the most relevant courses:\\n\")\n",
    "            \n",
    "            for detail in details:\n",
    "                sections.append(self._format_details(detail))\n",
    "        \n",
    "        return \"\\n\".join(sections)\n",
    "\n",
    "assembler = ContextAssembler()\n",
    "\n",
    "print(\"âœ… Context assembler created\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fcbb32-5e44-4ebb-b3d6-fbe89a7282c9",
   "metadata": {},
   "source": [
    "### Test Your Implementation\n",
    "\n",
    "Before connecting to the agent, let's verify your assembly strategies work correctly. Run the test utility below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa66821-e3f5-4031-b019-347c9097a904",
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_context_assembler import test_assembler\n",
    "\n",
    "# Test your implementation with real course data\n",
    "test_assembler(assembler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b364088d-255f-4934-bae0-84cdf8fa1a3c",
   "metadata": {},
   "source": [
    "### Connect to the agent\n",
    "\n",
    "Now that we've tested the implementation, let's connect it to the agent system. Run the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6fe8a9-9bc6-4c81-8e5a-b116604306b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect your implementation to the agent\n",
    "import redis_context_course.hierarchical_context as hc_module\n",
    "\n",
    "# Your ContextAssembler inherits from HierarchicalContextAssembler,\n",
    "# so it has the formatting helpers built-in and your custom assembly strategies.\n",
    "# We just need to tell the agent module to use your instance instead of the default.\n",
    "hc_module.context_assembler = assembler\n",
    "\n",
    "print(\"âœ… ContextAssembler is now connected to the agent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481c4ad2-d588-41ab-807e-3b74239f4d98",
   "metadata": {},
   "source": [
    "## Part 3: Intent Classification (Query-Aware Context Selection)\n",
    "\n",
    "We now have hierarchical data models (Part 1) and progressive disclosure assembly (Part 2). We're just missing the piece that allows the agent to adapt to the question being asked. This is where intent classification comes in.\n",
    "\n",
    "Here's the context engineering problem we're solving:\n",
    "\n",
    "If someone asks *\"What is CS101?\"* â†’ They need a summary (cheap, fast, ~100 tokens)  \n",
    "If someone asks *\"Show me the syllabus for CS101\"* â†’ They need full details (expensive, thorough, ~1000 tokens)\n",
    "\n",
    "Without intent classification, we'd face a context engineering dilemma:\n",
    "1. Always return full details â†’ wasteful, slow, expensive, and risks overwhelming the LLM with unnecessary information\n",
    "2. Always return summaries â†’ incomplete, frustrating, and fails to provide sufficient context for detailed queries\n",
    "\n",
    "Intent Classification is the decision layer in our agent's flow. It analyzes the query to determine what level of detail the LLM needs to generate a quality response.\n",
    "\n",
    "We'll use a system that routes queries based on five retrieval strategies:\n",
    "\n",
    "- GREETING â†’ Skip search entirely, return friendly response\n",
    "- GENERAL â†’ Return course summaries only (~500 tokens for 5 courses)\n",
    "- PREREQUISITES â†’ Return course summaries only (prerequisite codes are included in summaries)\n",
    "- SYLLABUS_OBJECTIVES â†’ Return summaries + full details (~2000 tokens)\n",
    "- ASSIGNMENTS â†’ Return summaries + full details (~2000 tokens)\n",
    "\n",
    "The classifier acts as an early filter. It determines whether to run the full agent workflow or short-circuit with a simple greeting response. This saves unnecessary API calls and retrieval operations for queries that don't need them.\n",
    "\n",
    "### ğŸ“Œ Task 2: Implement the Intent Classifier\n",
    "\n",
    "Your goal is to build a query classification function that analyzes user questions and determines the appropriate retrieval strategy. This classifier serves as the decision layer in your agent's workflow, routing queries to either skip search entirely (for greetings) or determining the depth of information needed (summaries vs. full details).\n",
    "\n",
    "The function will use an LLM to evaluate the query against five intent categories (GREETING, GENERAL, SYLLABUS_OBJECTIVES, ASSIGNMENTS, PREREQUISITES) and return the most appropriate category. We've provided the classification prompt with clear category definitions and examples.\n",
    "\n",
    "<details>\n",
    "<summary> ğŸ› ï¸ Show Implementation Details </summary>\n",
    "<br>\n",
    "\n",
    "\n",
    "1: **Get the LLM instance**\n",
    "\n",
    "Call `get_analysis_llm()` (already imported) and assign it to a variable named `llm`.\n",
    "\n",
    "2: **Send the prompt to the LLM**\n",
    "\n",
    "Use the `llm` variable to call `.ainvoke()` with a message list. Wrap the `intent_prompt` (already provided in the code) in a `HumanMessage` object: `[HumanMessage(content=intent_prompt)]`. \n",
    "\n",
    "Store the result in a variable named `response`.\n",
    "\n",
    "3: **Extract the text**\n",
    "\n",
    "Get the response text using `response.content.strip()` and store it in a variable named `response_content`.\n",
    "\n",
    "4: **Parse the intent**\n",
    "\n",
    "Loop through the lines in `response_content.split(\"\\n\")` to find the line that starts with `\"INTENT:\"`, then extract the category name after the colon using `.split(\":\", 1)[1].strip()`.\n",
    "\n",
    "Store the extracted intent in a variable named `intent`, defaulting to `\"GENERAL\"` if no match is found.\n",
    "\n",
    "</details>\n",
    "\n",
    "If you get stuck, reference the solution dropdown after the code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe75d28a-5bd6-488c-b0f9-961e62b4e672",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from agent.nodes import get_analysis_llm\n",
    "\n",
    "async def classify_intent_node(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Classify user query intent to determine appropriate retrieval strategy.\n",
    "    \n",
    "    Args:\n",
    "        query: The user's question\n",
    "        \n",
    "    Returns:\n",
    "        Intent category string (GREETING, GENERAL, SYLLABUS_OBJECTIVES, ASSIGNMENTS, PREREQUISITES)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Classification prompt (provided for you - defining good categories is the hard part!)\n",
    "    intent_prompt = f\"\"\"You are a query intent classifier for a course information system.\n",
    "\n",
    "TASK: Analyze the query and return ONLY the most appropriate intent category.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "INTENT CATEGORIES:\n",
    "\n",
    "1. GREETING\n",
    "   - Greetings, acknowledgments, pleasantries\n",
    "   - Examples: \"hello\", \"hi there\", \"thank you\", \"thanks\"\n",
    "\n",
    "2. GENERAL\n",
    "   - Broad course information requests\n",
    "   - Course descriptions and overviews\n",
    "   - \"What is [course]?\" questions\n",
    "   - Example: \"What is CS002?\"\n",
    "\n",
    "3. SYLLABUS_OBJECTIVES\n",
    "   - Syllabus requests\n",
    "   - Course structure and topics covered\n",
    "   - Learning objectives and outcomes\n",
    "   - Examples: \"Show me the syllabus for CS002\", \"What will I learn?\", \"What topics are covered?\", \"Give me details about this course\"\n",
    "\n",
    "4. ASSIGNMENTS\n",
    "   - Homework, projects, exams\n",
    "   - Assessment types and workload\n",
    "   - Grading information\n",
    "   - Examples: \"What are the assignments?\", \"How many exams?\", \"What's the workload?\"\n",
    "\n",
    "5. PREREQUISITES\n",
    "   - Course requirements\n",
    "   - Prior knowledge needed\n",
    "   - Examples: \"What are the prerequisites?\", \"What do I need before taking this?\"\n",
    "\n",
    "CLASSIFICATION RULES:\n",
    "- Choose the MOST SPECIFIC category that matches\n",
    "- If multiple categories apply, prioritize based on the primary intent\n",
    "- Default to GENERAL for ambiguous queries\n",
    "- Ignore filler words and focus on core intent\n",
    "\n",
    "OUTPUT FORMAT (respond with exactly this structure):\n",
    "INTENT: <category_name>\n",
    "\"\"\"\n",
    "    \n",
    "    # TODO: Step 1 - Get the LLM instance\n",
    "    \n",
    "    # TODO: Step 2 - Send the prompt to the LLM\n",
    "    \n",
    "    # TODO: Step 3 - Extract the response text\n",
    "    \n",
    "    # TODO: Step 4 - Parse the intent from the response\n",
    "    \n",
    "    return intent\n",
    "\n",
    "print(\"âœ… Intent classification node created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9bab6f-6183-4786-8233-6b443feb8e7d",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ğŸ—ï¸ Solution code</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from agent.nodes import get_analysis_llm\n",
    "\n",
    "async def classify_intent_node(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Classify user query intent to determine appropriate retrieval strategy.\n",
    "    \n",
    "    Args:\n",
    "        query: The user's question\n",
    "        \n",
    "    Returns:\n",
    "        Intent category string (GREETING, GENERAL, SYLLABUS_OBJECTIVES, ASSIGNMENTS, PREREQUISITES)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Classification prompt (provided for you - defining good categories is the hard part!)\n",
    "    intent_prompt = f\"\"\"You are a query intent classifier for a course information system.\n",
    "\n",
    "TASK: Analyze the query and return ONLY the most appropriate intent category.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "INTENT CATEGORIES:\n",
    "\n",
    "1. GREETING\n",
    "   - Greetings, acknowledgments, pleasantries\n",
    "   - Examples: \"hello\", \"hi there\", \"thank you\", \"thanks\"\n",
    "\n",
    "2. GENERAL\n",
    "   - Broad course information requests\n",
    "   - Course descriptions and overviews\n",
    "   - \"What is [course]?\" questions\n",
    "   - Example: \"What is CS002?\"\n",
    "\n",
    "3. SYLLABUS_OBJECTIVES\n",
    "   - Syllabus requests\n",
    "   - Course structure and topics covered\n",
    "   - Learning objectives and outcomes\n",
    "   - Examples: \"Show me the syllabus for CS002\", \"What will I learn?\", \"What topics are covered?\", \"Give me details about this course\"\n",
    "\n",
    "4. ASSIGNMENTS\n",
    "   - Homework, projects, exams\n",
    "   - Assessment types and workload\n",
    "   - Grading information\n",
    "   - Examples: \"What are the assignments?\", \"How many exams?\", \"What's the workload?\"\n",
    "\n",
    "5. PREREQUISITES\n",
    "   - Course requirements\n",
    "   - Prior knowledge needed\n",
    "   - Examples: \"What are the prerequisites?\", \"What do I need before taking this?\"\n",
    "\n",
    "CLASSIFICATION RULES:\n",
    "- Choose the MOST SPECIFIC category that matches\n",
    "- If multiple categories apply, prioritize based on the primary intent\n",
    "- Default to GENERAL for ambiguous queries\n",
    "- Ignore filler words and focus on core intent\n",
    "\n",
    "OUTPUT FORMAT (respond with exactly this structure):\n",
    "INTENT: <category_name>\n",
    "\"\"\"\n",
    "    \n",
    "    # Step 1 - Get the LLM instance\n",
    "    llm = get_analysis_llm()\n",
    "    \n",
    "    # Step 2 - Send the prompt to the LLM\n",
    "    response = await llm.ainvoke([HumanMessage(content=intent_prompt)])\n",
    "    \n",
    "    # Step 3 - Extract the response text\n",
    "    response_content = response.content.strip()\n",
    "    \n",
    "    # Step 4 - Parse the intent from the response\n",
    "    intent = \"GENERAL\"  # Default fallback\n",
    "    for line in response_content.split(\"\\n\"):\n",
    "        if line.startswith(\"INTENT:\"):\n",
    "            intent = line.split(\":\", 1)[1].strip()\n",
    "            break\n",
    "    \n",
    "    return intent\n",
    "\n",
    "print(\"âœ… Intent classification node created\")\n",
    "```\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4a1b2a-a08d-4af8-939c-9ca0b962f660",
   "metadata": {},
   "source": [
    "### Test Your Implementation\n",
    "\n",
    "Now let's test your intent classifier with the test utility. It will run 17 test cases covering all 5 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366e2335-de0c-41be-bd93-7c0a420922f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_intent_classifier import test_intent_classifier\n",
    "\n",
    "# Run tests\n",
    "await test_intent_classifier(classify_intent_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95190e9a-312b-4911-840f-7cc72ec61003",
   "metadata": {},
   "source": [
    "### Connect to the Agent\n",
    "\n",
    "Now let's inject your implementation into the Stage 3 agent. This replaces the default classification logic in `classify_intent_node` with your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850fa2a9-56ff-4a37-b34f-26595cefb7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the agent module\n",
    "from agent import set_classify_intent_function\n",
    "\n",
    "# Inject your implementation\n",
    "set_classify_intent_function(classify_intent_node)\n",
    "\n",
    "print(\"âœ… Your intent classifier has been injected into the agent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4f5dc9-a7b8-4669-9947-dcb8e0cfa5a2",
   "metadata": {},
   "source": [
    "## Part 4: The Search Tool\n",
    "\n",
    "We now have hierarchical data models (Part 1), the progressive disclosure assembly (Part 2), and a intent classification node (Part 3). In this section, we will see how we define a tool to process the intent (returned by our classification node) and process the context assembly to return the right amount of *depth* based on the intent of the query. \n",
    "\n",
    "If you're unfamiliar with tools, a tool (we are using LangChain to define one) is simply a function that the LLM can invoke to perform a specific task. To make our retrieval logic accessible to the agent, we need to define two things:\n",
    "1.  **The Input Schema (`SearchCoursesInput`)**: A Pydantic model that tells the LLM *what arguments* it can provide.\n",
    "2.  **The Tool Function (`search_courses_tool`)**: The actual Python function that executes the logic.\n",
    "\n",
    "In the agent code, the complex logic for connecting to Redis and assembling the context is already defined by a function called `search_courses_sync` (located in `agent/tools.py`). This function connects the intent (determined by the LLM via the intent classifier node) to the context assembler. For example, here is a snippet from that function: \n",
    "\n",
    "```python\n",
    "# Inside search_courses_sync:\n",
    "if intent == \"GENERAL\":\n",
    "    # Use the assembler to return summaries only\n",
    "    return context_assembler.assemble_summary_only_context(...)\n",
    "else:\n",
    "    # Use the assembler to return full details\n",
    "    return context_assembler.assemble_hierarchical_context(...)\n",
    "```\n",
    "\n",
    "#### ğŸ“Œ **Task: Define the Search Courses Tool**\n",
    "\n",
    "Your task is to define the `SearchCoursesInput` schema and the `search_courses_tool` function.\n",
    "\n",
    "In the starter code below, you'll find we have imported the following:\n",
    "- The `BaseModel` and `Field` class from pydantic. These are used to define structured input schemas. Don't worry if you haven't used Pydantic before. Think of it as defining a form that the LLM has to fill out. The most important part is the `description` field. It tells the LLM what each field means and what values are allowed.\n",
    "- A `tool` decorator from `langchain_core.tools`. This converts our Python function into a LangChain tool\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ› ï¸ Show Implementation Details</summary>\n",
    "\n",
    "1. **Define the intent field in the input schema (`SearchCoursesInput`)**:\n",
    "\n",
    "   Create the intent field. You must list the valid categories (GENERAL, PREREQUISITES, SYLLABUS_OBJECTIVES, ASSIGNMENTS) in the description so the LLM knows which one to pick.\n",
    "\n",
    "2. **Add the Tool Decorator**:\n",
    "\n",
    "   Add the LangChain `@tool` decorator to the function. The tool needs to have two arguments passed into it: the tool name (`\"search_courses\"`) and the schema (`SearchCoursesInput`).\n",
    "\n",
    "3. **Call the pre-defined retrieval function in the tool (`search_courses_tool`)**:\n",
    "\n",
    "   Call the `search_courses_sync` function inside the `try` block with the correct parameters:\n",
    "    - `query`: The user's query\n",
    "    - `intent`: The intent selected by the LLM\n",
    "    - `top_k`: Set to `5`\n",
    "    - `use_optimized_format`: Set to `False` (we are using hierarchical format now)\n",
    "      \n",
    "</details>\n",
    "\n",
    "If you get stuck, reference the solution dropdown below the implementation cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f97f972-2281-47b3-a955-4d49167a752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.tools import tool\n",
    "from agent.tools import search_courses_sync\n",
    "\n",
    "\n",
    "# Define the Input Schema (provided for you - defining good schemas is the hard part!)\n",
    "class SearchCoursesInput(BaseModel):\n",
    "    \"\"\"Input schema for search_courses tool.\"\"\"\n",
    "\n",
    "    query: str = Field(description=\"The search query for finding courses\")\n",
    "    \n",
    "    # TODO: Define the 'intent' field\n",
    "    # Use Field() with default=\"GENERAL\" and a description listing valid categories:\n",
    "    # GENERAL, PREREQUISITES, SYLLABUS_OBJECTIVES, ASSIGNMENTS\n",
    "\n",
    "\n",
    "# TODO: Add the @tool decorator\n",
    "# Pass the tool name \"search_courses\" and args_schema=SearchCoursesInput\n",
    "def search_courses_tool(query: str, intent: str = \"GENERAL\") -> str:\n",
    "    \"\"\"\n",
    "    Search for courses with intent-based hierarchical retrieval.\n",
    "    \n",
    "    Args:\n",
    "        query: The user's search query\n",
    "        intent: The intent category (GENERAL, PREREQUISITES, SYLLABUS_OBJECTIVES, ASSIGNMENTS)\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted context with course information\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Check if course_manager is initialized\n",
    "    # If not, return \"Course search not available - CourseManager not initialized\"\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        # TODO: Call search_courses_sync\n",
    "        # Call with: query, top_k=5, use_optimized_format=False, intent\n",
    "        # Store result in 'result' variable\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Search failed: {str(e)}\"\n",
    "\n",
    "print(\"âœ… Tool schema defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f0d0be-8268-4a5e-9964-2314460438b4",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ğŸ—ï¸ Solution code</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.tools import tool\n",
    "from agent.tools import search_courses_sync\n",
    "\n",
    "# Step 1 - Define the Input Schema\n",
    "class SearchCoursesInput(BaseModel):\n",
    "    \"\"\"Input schema for search_courses tool.\"\"\"\n",
    "\n",
    "    query: str = Field(description=\"The search query for finding courses\")\n",
    "    intent: str = Field(\n",
    "        default=\"GENERAL\",\n",
    "        description=\"Intent category: GENERAL (summaries only), PREREQUISITES (summaries only - prereq codes included), SYLLABUS_OBJECTIVES (full details), ASSIGNMENTS (full details)\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Step 2 - Define the Tool Function\n",
    "@tool(\"search_courses\", args_schema=SearchCoursesInput)\n",
    "async def search_courses_tool(query: str, intent: str = \"GENERAL\") -> str:\n",
    "    \"\"\"\n",
    "    Search for courses with intent-based hierarchical retrieval.\n",
    "    \"\"\"\n",
    "    # Check if dependencies are ready (good practice)\n",
    "    if not course_manager:\n",
    "        return \"Course search not available - CourseManager not initialized\"\n",
    "\n",
    "    try:\n",
    "        result = search_courses_sync(\n",
    "            query=query,\n",
    "            top_k=5,\n",
    "            use_optimized_format=False,\n",
    "            intent=intent,\n",
    "        )\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return f\"Search failed: {str(e)}\"\n",
    "\n",
    "print(\"âœ… Tool defined with intent-based retrieval!\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc3a5cc-a9fe-4fbc-bb78-ba460a79d68c",
   "metadata": {},
   "source": [
    "### Test Your Implementation\n",
    "\n",
    "Run the test utility below to verify your tool works correctly with different intents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb172a4-d2cc-4b49-8323-bf2192830186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the test utility\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from test_search_tool import test_search_courses_tool\n",
    "\n",
    "# Test your implementation\n",
    "await test_search_courses_tool(search_courses_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb39dcf-5704-4159-b54e-760772b73b8a",
   "metadata": {},
   "source": [
    "### Connect to the Agent\n",
    "\n",
    "Now let's inject your tool implementation into the Stage 3 agent. This replaces the default search tool in the `agent_node` with your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab894bc-fc75-4e6b-8ead-9e52e889551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the agent module\n",
    "from agent import set_search_tool\n",
    "\n",
    "# Inject your implementation\n",
    "set_search_tool(search_courses_tool)\n",
    "\n",
    "print(\"âœ… Your search tool has been injected into the agent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddf11d8-60db-429f-965f-f2c465c7bf1c",
   "metadata": {},
   "source": [
    "## Part 5: Quality Evaluation (Context Validation)\n",
    "\n",
    "We now have all the pieces to retrieve context intelligently (hierarchical data, intent classification, tool calling). But we're missing something critical: validation.\n",
    "\n",
    "According to LangChain's [2025 State of Agent Engineering survey](https://www.langchain.com/state-of-agent-engineering#biggest-barriers-to-production), output quality and reliability remain the #1 barrier to putting agents into production. Teams can build sophisticated retrieval pipelines, but if they can't consistently validate the quality of what they're retrieving, they can't trust their systems in production.\n",
    "\n",
    "What if the search returns irrelevant results? What if the retrieved context doesn't actually answer the question? In a production system, we can't afford to send poor-quality context to the LLM and hope for the best.\n",
    "\n",
    "This is where quality evaluation comes in. It's a validation gate that checks: *\"Is the retrieved context good enough?\"*\n",
    "\n",
    "To track quality in our agent,  we'll use a technique called LLM-as-a-Judge, one of the most widely adopted evaluation techniques (according to the same LangChain survey). \n",
    "\n",
    "If you're unfamiliar with the LLM-as-a-Judge technique, it's essentially using a separate LLM call specifically designed to evaluate quality against explicit criteria.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "After our system retrieves context, we ask an LLM (serving the role of an evaluator) to score it on four dimensions:\n",
    "- **Completeness**: Does this context fully answer the question?\n",
    "- **Accuracy**: Is the information correct and relevant?\n",
    "- **Relevance**: Does it directly address what was asked?\n",
    "- **Grounding**: Does it reference specific courses (not generic knowledge)?\n",
    "\n",
    "The evaluator returns a quality score (0.0 to 1.0):\n",
    "- **Score â‰¥ 0.7**: \"Good enough\" â†’ Proceed to synthesis\n",
    "- **Score < 0.7**: \"Insufficient\" â†’ Trigger another search\n",
    "\n",
    "This creates a feedback loop where poor results automatically trigger re-retrieval.\n",
    "\n",
    "Let's now get started implementing it into the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60315f9f-cb9a-46e3-ba35-0fadb1f7c51d",
   "metadata": {},
   "source": [
    "### ğŸ“Œ Task: Build the Quality Evaluator\n",
    "\n",
    "Your task is to implement the `evaluate_context_quality` function, which determines whether the retrieved context is sufficient to answer the user's question.\n",
    "\n",
    "You'll build a function that takes a `question` and `context` as input, uses an LLM to evaluate the context quality on 4 criteria (Completeness, Accuracy, Relevance, Grounding), and returns a tuple: `(score: float, reasoning: str)` where score is between 0.0 and 1.0.\n",
    "\n",
    "The implementation cell below includes starter code with a pre-written evaluation prompt that instructs the LLM on the 4 criteria, and tasks to guide your implementation.\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ› ï¸ Show Detailed Implementation Steps</summary>\n",
    "    \n",
    "1. **Get the LLM Instance**\n",
    "\n",
    "   The agent uses a pre-configured LLM for analysis tasks. Call `get_analysis_llm()` and assign it to the `llm` variable (already created in starter code).\n",
    "\n",
    "2. **Send Prompt to LLM**\n",
    "\n",
    "   Use the `llm` variable from the first step to call `.ainvoke()` with a message list. Wrap the `evaluation_prompt` (already provided) in a `HumanMessage` object (already imported): `[HumanMessage(content=evaluation_prompt)]`. Store the result in the `response` variable (already created in starter code).\n",
    "\n",
    "3. **Extract and Parse the Score**\n",
    "\n",
    "   Extract the text from `response.content` and strip whitespace, storing it in the `score_text` variable. Then, convert `score_text` to a float and store it in the `score` variable. Lastly, clamp `score` between 0.0 and 1.0 using `max(0.0, min(1.0, score))`\n",
    "\n",
    "4. **Generate Reasoning**\n",
    "\n",
    "   Use the `score` variable to create a human-readable explanation and store it in the `reasoning` variable. The threshold for \"adequate\" quality is 0.7 - scores at or above this are good enough to proceed.\n",
    "\n",
    "5. **Error Handling**\n",
    "\n",
    "   The try/except structure is already provided. In the except block, if parsing fails (ValueError), assign safe defaults to the variables: `score = 0.8` and `reasoning = \"âš ï¸ Parsing error, defaulting to 0.8\"`.\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "If you get stuck, reference the full solution dropdown below the test section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7be78c2-70e4-464b-a6aa-b741c1c0de50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from agent.nodes import get_analysis_llm\n",
    "\n",
    "async def evaluate_context_quality(question: str, context: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Evaluate the quality of retrieved context.\n",
    "    \n",
    "    Args:\n",
    "        question: The user's original question\n",
    "        context: The retrieved context to evaluate\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (score: float between 0.0-1.0, reasoning: str)\n",
    "    \n",
    "    This function implements the quality evaluation gate that decides\n",
    "    whether context is good enough to use or if we need to search again.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Starter variables\n",
    "    llm = None\n",
    "    response = None\n",
    "    score_text = \"\"\n",
    "    score = 0.0\n",
    "    reasoning = \"\"\n",
    "    \n",
    "    # Evaluation prompt (already provided for you!)\n",
    "    evaluation_prompt = f\"\"\"Evaluate the quality of this course search answer on a scale of 0.0 to 1.0.\n",
    "\n",
    "Question: {question}\n",
    "Answer: {context}\n",
    "\n",
    "Criteria:\n",
    "- Completeness: Does it fully answer the question?\n",
    "- Accuracy: Is the course information correct and relevant?\n",
    "- Relevance: Does it directly address what was asked?\n",
    "- Grounding: Does it provide specific course details and stick to facts?\n",
    "\n",
    "Respond with ONLY a number between 0.0 and 1.0 (e.g., 0.85)\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # TODO: Get the LLM instance\n",
    "        \n",
    "        # TODO: Send prompt to LLM with HumanMessage\n",
    "        \n",
    "        # TODO: Extract response content and parse as float\n",
    "        \n",
    "        # TODO: Clamp score between 0.0 and 1.0\n",
    "        \n",
    "        # Generate reasoning based on score\n",
    "        if score >= 0.7:\n",
    "            reasoning = f\"âœ… Adequate quality (score: {score:.2f})\"\n",
    "        else:\n",
    "            reasoning = f\"âš ï¸ Needs improvement (score: {score:.2f})\"\n",
    "        \n",
    "    except ValueError:\n",
    "        # Handle parsing errors with safe defaults\n",
    "        score = 0.8\n",
    "        reasoning = \"âš ï¸ Parsing error, defaulting to 0.8\"\n",
    "    \n",
    "    return score, reasoning\n",
    "\n",
    "print(\"âœ… Evaluation function created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bee8b5-ca1c-4dab-a871-a6a96ee1a1df",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ğŸ—ï¸ Solution code</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from agent.nodes import get_analysis_llm\n",
    "\n",
    "async def evaluate_context_quality(question: str, context: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Evaluate the quality of retrieved context.\n",
    "    \n",
    "    Args:\n",
    "        question: The user's original question\n",
    "        context: The retrieved context to evaluate\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (score: float between 0.0-1.0, reasoning: str)\n",
    "    \n",
    "    This function implements the quality evaluation gate that decides\n",
    "    whether context is good enough to use or if we need to search again.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Starter variables\n",
    "    llm = None\n",
    "    response = None\n",
    "    score_text = \"\"\n",
    "    score = 0.0\n",
    "    reasoning = \"\"\n",
    "    \n",
    "    # Evaluation prompt (already provided for you!)\n",
    "    evaluation_prompt = f\"\"\"Evaluate the quality of this course search answer on a scale of 0.0 to 1.0.\n",
    "\n",
    "Question: {question}\n",
    "Answer: {context}\n",
    "\n",
    "Criteria:\n",
    "- Completeness: Does it fully answer the question?\n",
    "- Accuracy: Is the course information correct and relevant?\n",
    "- Relevance: Does it directly address what was asked?\n",
    "- Grounding: Does it provide specific course details and stick to facts?\n",
    "\n",
    "Respond with ONLY a number between 0.0 and 1.0 (e.g., 0.85)\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # TODO 1: Get the LLM instance\n",
    "        llm = get_analysis_llm()\n",
    "        \n",
    "        # TODO 2: Send prompt to LLM\n",
    "        response = await llm.ainvoke([HumanMessage(content=evaluation_prompt)])\n",
    "        \n",
    "        # TODO 3: Extract and parse the score\n",
    "        score_text = response.content.strip()\n",
    "        score = float(score_text)\n",
    "        score = max(0.0, min(1.0, score))\n",
    "        \n",
    "        # TODO 4: Generate reasoning\n",
    "        if score >= 0.7:\n",
    "            reasoning = f\"âœ… Adequate quality (score: {score:.2f})\"\n",
    "        else:\n",
    "            reasoning = f\"âš ï¸ Needs improvement (score: {score:.2f})\"\n",
    "        \n",
    "    except ValueError:\n",
    "        # TODO 5: Error handling\n",
    "        score = 0.8\n",
    "        reasoning = \"âš ï¸ Parsing error, defaulting to 0.8\"\n",
    "    \n",
    "    return score, reasoning\n",
    "\n",
    "print(\"âœ… Evaluation function created!\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e356b0-3313-4ba5-810c-beb93d4370d3",
   "metadata": {},
   "source": [
    "### Test Your Implementation\n",
    "\n",
    "Run the test utility below to verify your quality evaluator works correctly with different context quality levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e81104-efb9-4dfc-9f02-947ab73cd9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the test utility\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from test_quality_evaluator import test_quality_evaluator\n",
    "\n",
    "# Test your implementation\n",
    "await test_quality_evaluator(evaluate_context_quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504fdd75-10a4-45bf-83b0-401270024dbe",
   "metadata": {},
   "source": [
    "### Connect to the Agent\n",
    "\n",
    "Now, let's inject your quality evaluator into the Stage 3 agent. This replaces the default evaluation logic in `evaluate_quality_node` with your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874d935e-6f21-499f-96c8-8b0ba88a9ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the agent module\n",
    "from agent import set_evaluate_quality_function\n",
    "\n",
    "# Inject your implementation\n",
    "set_evaluate_quality_function(evaluate_context_quality)\n",
    "\n",
    "print(\"âœ… Your quality evaluator has been injected into the agent!\")\n",
    "print(\"ğŸ¯ The evaluate_quality_node will now use your implementation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc753ef-ea2e-4daa-97bf-c2d87486135f",
   "metadata": {},
   "source": [
    "## Part 6: Running the complete agent\n",
    "\n",
    "Great job. At this point you have explored and built the five core components of the Stage 3 agent:\n",
    "\n",
    "1. The hierarchical data models (CourseSummary & CourseDetails)  \n",
    "2. The Context assembler with progressive disclosure  \n",
    "3. The intent classifier for query-aware routing  \n",
    "4. The Search tool with intent-based retrieval  \n",
    "5. The Quality evaluator using a LLM-as-a-Judge pattern\n",
    "\n",
    "Now it's time to see everything work together in the full workflow. This workflow includes one additional component we haven't explicitly built: a greeting handler node that responds to pleasantries without triggering any search or retrieval (pure cost optimization). You can find the code for this pre-built node in the `nodes.py` file of the agent.\n",
    "\n",
    "Let's now run the full agent and test it with a variety of queries to see how your implementations handle different scenarios and optimize token usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8f78dd-7a78-4e25-839e-0714394fe8d8",
   "metadata": {},
   "source": [
    "### Test 1: Greeting Query (Zero-Cost Route)\n",
    "\n",
    "First, let's test the greeting handler. The intent classifier will identify this as a GREETING, and the workflow will route directly to the greeting nodeâ€”skipping all retrieval, tool calling, and quality evaluation.\n",
    "\n",
    "Expected behavior:\n",
    "- Intent: GREETING\n",
    "- Route: classify_intent â†’ handle_greeting â†’ END\n",
    "- Retrieval: None\n",
    "- Token cost: ~100 (just the greeting response)\n",
    "\n",
    "Run the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0193de-6fdb-4a1b-a903-222540e2a33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the test utility to run tests\n",
    "from test_util import run_agent_test\n",
    "\n",
    "# Query 1: Greeting (tests the greeting handler)\n",
    "query1 = \"Hello! How are you doing?\"\n",
    "await run_agent_test(workflow, query1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73009612-fd64-447f-9e39-49f94d036ac7",
   "metadata": {},
   "source": [
    "### Test 2: General Query (Summary-Only Retrieval)\n",
    "\n",
    "Now, let's test a **GENERAL** intent query. Your intent classifier will detect the need for course overviews, and your search tool will retrieve summaries only (not full details).\n",
    "\n",
    "Expected behavior:\n",
    "- Intent: GENERAL\n",
    "- Route: classify_intent â†’ agent â†’ search_courses_tool (summary mode) â†’ evaluate_quality â†’ synthesize\n",
    "- Retrieval: CourseSummary objects only\n",
    "- Token cost: ~1500\n",
    "\n",
    "Run the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baf7b49-e7cf-4783-90c6-22cc2e06ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 2: General question (low cost, summaries only)\n",
    "query2 = \"What computer science courses are available?\"\n",
    "await run_agent_test(workflow, query2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f99a9b7-dfbb-46d1-b6c6-56b346ccf958",
   "metadata": {},
   "source": [
    "### Test 3: Detailed Query (Hierarchical Retrieval)\n",
    "\n",
    "Now a **SYLLABUS_OBJECTIVES** query. With this query, the intent classifier will detect that it requires detailed information, and the search tool will utilize hierarchical retrieval (summaries and full details with progressive disclosure).\n",
    "\n",
    "Expected behavior:\n",
    "- Intent: SYLLABUS_OBJECTIVES\n",
    "- Route: classify_intent â†’ agent â†’ search_courses_tool (hierarchical mode) â†’ evaluate_quality â†’ synthesize\n",
    "- Retrieval: 5 CourseSummary + 2-3 CourseDetails with full syllabi\n",
    "- Token cost: ~4000-5000 (summaries + detailed syllabi + LLM generation)\n",
    "  \n",
    "Run the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef1734d-a55f-4344-b0a2-c9d5a3eb963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 3: Detailed question (hierarchical retrieval)\n",
    "query3 = \"What are the learning objectives for CS002?\"\n",
    "await run_agent_test(workflow, query3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8d8860-6ded-4f44-bb3b-48d63d4dbd72",
   "metadata": {},
   "source": [
    "### Test 4: Assignment Query (Another Detailed Scenario)\n",
    "\n",
    "Let's additionally test an **ASSIGNMENTS** intent to see hierarchical retrieval in action again.\n",
    "\n",
    "Expected behavior:\n",
    "- Intent: ASSIGNMENTS\n",
    "- Route: classify_intent â†’ agent â†’ search_courses_tool (hierarchical mode) â†’ evaluate_quality â†’ synthesize\n",
    "- Retrieval: 5 CourseSummary + 2-3 CourseDetails with full syllabi\n",
    "- Token cost: ~4000-5000 (summaries + detailed syllabi + LLM generation)\n",
    "  \n",
    "Run the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01680540-732a-4b56-b671-f61c64d28676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 4: Assignment-specific question\n",
    "query4 = \"What assignments are in CS002?\"\n",
    "await run_agent_test(workflow, query4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da581296-7dab-41a9-9656-a5aefa576590",
   "metadata": {},
   "source": [
    "### Final Comparison: Adaptive Retrieval in Action\n",
    "\n",
    "Now let's run all four test queries to see how your hierarchical retrieval system adapts to different query types. This will demonstrate the token savings and quality improvements from intent-driven context selection.\n",
    "\n",
    "We'll test:\n",
    "1. **Greeting** â†’ No retrieval (minimal tokens)\n",
    "2. **General overview** â†’ Summaries only (~1500 tokens)\n",
    "3. **Learning objectives** â†’ Hierarchical retrieval (~4000-5000 tokens)\n",
    "4. **Assignments** â†’ Hierarchical retrieval (~4000-5000 tokens)\n",
    "\n",
    "Run the code block below to see the full comparison and token analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225ef53d-3d47-41b2-97e5-3e5ac2471244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the comparison test function\n",
    "from test_comparison import run_comparison_tests\n",
    "\n",
    "# Run the comparison (pass the already-created workflow)\n",
    "await run_comparison_tests(workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164c5c3b",
   "metadata": {},
   "source": [
    "## Looking Ahead: The Limits of Semantic Search\n",
    "\n",
    "With hierarchical retrieval and intent classification, we've made significant progress in shaping *what* information reaches the LLM and *how much* of it. But context engineering isn't just about structure and volumeâ€”it's also about precision. Before we wrap up, let's examine a subtle limitation in how we *select* context.\n",
    "\n",
    "When users ask about a specific course code, vector search finds it through semantic similarity - not guaranteed exact matching. Run this experiment to see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c83e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: How reliably does vector search find an exact course code?\n",
    "from redisvl.query import VectorQuery\n",
    "import re\n",
    "\n",
    "test_queries = [\n",
    "    \"What are the learning objectives for CS002?\",\n",
    "    \"Tell me about CS014\",\n",
    "    \"What are the prerequisites for ARCH050?\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    query_embedding = await course_manager.embeddings.aembed_query(query)\n",
    "    \n",
    "    vector_query = VectorQuery(\n",
    "        vector=query_embedding,\n",
    "        vector_field_name=\"content_vector\",\n",
    "        return_fields=[\"course_code\", \"title\"],\n",
    "        num_results=5,\n",
    "    )\n",
    "    \n",
    "    results = course_manager.vector_index.query(vector_query)\n",
    "    result_list = results if isinstance(results, list) else results.docs\n",
    "    \n",
    "    # Extract the course code from the query\n",
    "    mentioned_code = re.search(r'([A-Z]{2,4}\\d{3})', query)\n",
    "    target_code = mentioned_code.group(1) if mentioned_code else \"?\"\n",
    "    \n",
    "    codes = [getattr(r, 'course_code', r.get('course_code', '')) if isinstance(r, dict) else getattr(r, 'course_code', '') for r in result_list]\n",
    "    \n",
    "    rank = codes.index(target_code) + 1 if target_code in codes else \"Not in top 5\"\n",
    "    \n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"  Target: {target_code} â†’ Rank: {rank}\")\n",
    "    print(f\"  Top 3: {codes[:3]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ee7358",
   "metadata": {},
   "source": [
    "Notice how the target course isn't always the #1 result. Sometimes it's 2nd, 3rd, or even lower. When a user explicitly mentions \"CS002\", they expect information about CS002, not whatever course happens to be semantically closest to their phrasing. Relying on \"semantic luck\" to surface the right course creates an unpredictable user experience. In Stage 4, we'll introduce hybrid search, a way of searching that combines exact matching with semantic search, ensuring that when users mention specific course codes, we find exactly what they asked for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dc5f5c-df3c-4c37-a62d-9606fef62319",
   "metadata": {},
   "source": [
    "## Wrap Up ğŸ\n",
    "\n",
    "You've completed Stage 3 and transformed your RAG system from static retrieval into an adaptive, intent-driven agent.\n",
    "\n",
    "In this stage, you learned you:\n",
    "\n",
    "- Implemented the Context Assembler that provides different levels of detail (summaries vs. full course details) based on query requirements\n",
    "- Implemented the Intent Classifier that analyzes queries and routes them to appropriate retrieval strategies\n",
    "- Created the Search Tool that combines intent classification with progressive disclosure (breadth-first, then depth)\n",
    "- Used the Quality Evaluator with LLM-as-a-Judge to ensure responses meet quality thresholds before delivery\n",
    "\n",
    "The key transformation: your agent now adapts retrieval depth to query complexity, achieving a ~30-50% token reduction while maintaining quality.\n",
    "\n",
    "In Stage 4, you'll level up with hybrid search (combining semantic + exact matching) and the ReAct agent architecture (giving your agent visible reasoning and tool-calling capabilities)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
