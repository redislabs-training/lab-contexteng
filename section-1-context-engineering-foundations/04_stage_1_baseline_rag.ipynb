{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6735a9ec",
   "metadata": {},
   "source": [
    "# Stage 1: The Baseline RAG Agent (Information Overload)\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/redislabs-training/ce-redis-langchain/blob/main/section-1-context-engineering-foundations/04_stage_1_baseline_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to the first stage of the course. Here, we'll begin by exploring context through the lens of a basic RAG agent. More specifically, we'll do the following:\n",
    "\n",
    "1.  Explore the baseline architecture of the simple RAG agent. Our Stage 1 Agent takes the simplest possible approach: Find relevant courses and give the LLM *everything* about them.\n",
    "2.  Witness \"Information Overload\": See firsthand what happens when you retrieve *too much* context.\n",
    "3.  Analyze the Cost: Measure the token usage and latency of a naive approach.\n",
    "\n",
    "Let's now dive in by going over how the agent works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e610a6e9",
   "metadata": {},
   "source": [
    "## Agent Overview\n",
    "\n",
    "The code for the baseline agent lives in `progressive_agents/stage1_baseline_rag/`. You can reference the code at any time throughout this lesson. The agent has three important components:\n",
    "\n",
    "### 1. LangGraph Nodes (`agent/nodes.py`)\n",
    "The logic is split into two functions (nodes):\n",
    "*   `research_node`:\n",
    "    *   Searches Redis for the top 5 courses matching the user's query.\n",
    "    *   The Flaw: It retrieves the FULL hierarchical data for all 5 courses. This includes every single week of the syllabus, every homework assignment, and every reading list.\n",
    "*   `synthesize_node`:\n",
    "    *   Receives this massive block of text.\n",
    "    *   Sends it all to the LLM with a prompt to answer the user's question.\n",
    "\n",
    "### 2. The \"Memory\": State (`agent/state.py`)\n",
    "We use a simple `TypedDict` to pass data between nodes.\n",
    "```python\n",
    "class AgentState(TypedDict):\n",
    "    query: str              # The user's question\n",
    "    raw_context: str        # The massive JSON blob of course data\n",
    "    final_answer: str       # The LLM's response\n",
    "    total_tokens: int       # Tracking our inefficiency\n",
    "```\n",
    "\n",
    "### 3. The \"Blueprint\": Workflow (`agent/workflow.py`)\n",
    "We use LangGraph to orchestrate the flow. It's a simple linear graph:\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    START([Start]) --> Research[Research Node]\n",
    "    Research --> Synthesize[Synthesize Node]\n",
    "    Synthesize --> END([End])\n",
    "    \n",
    "    style Research fill:#ff9999,stroke:#333,stroke-width:2px\n",
    "    style Synthesize fill:#99ccff,stroke:#333,stroke-width:2px\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344c64d1",
   "metadata": {},
   "source": [
    "# Setup and Initialization\n",
    "\n",
    "First, let's set up our environment. We need to import the agent code from the `progressive_agents` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0db6228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 1. Configure Paths\n",
    "# We are currently in 'section-1...', so we look up two levels to find 'progressive_agents'\n",
    "project_root = Path(\"../../\").resolve()\n",
    "stage1_path = project_root / \"progressive_agents\" / \"stage1_baseline_rag\"\n",
    "sys.path.append(str(stage1_path))\n",
    "\n",
    "# 2. Load Environment Variables (API Keys, Redis URL)\n",
    "load_dotenv(project_root / \".env\")\n",
    "\n",
    "print(f\"Project Root: {project_root}\")\n",
    "print(f\"Agent Path Added: {stage1_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e92bb0b",
   "metadata": {},
   "source": [
    "### Initialize the Agent\n",
    "We will use the `setup_agent` helper. This function performs a crucial step:\n",
    "*   It connects to your Redis instance.\n",
    "*   It checks if the course data exists.\n",
    "*   If not, it generates 50 sample courses and loads them into Redis.\n",
    "\n",
    "*Note: This might take a few seconds the first time you run it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c77682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import setup_agent\n",
    "\n",
    "print(\"Initializing Stage 1 Agent...\")\n",
    "# auto_load_courses=True ensures we have data to query\n",
    "workflow, course_manager = setup_agent(auto_load_courses=True)\n",
    "print(\"Agent is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7356857",
   "metadata": {},
   "source": [
    "## The Experiment: \"What ML courses are available?\"\n",
    "\n",
    "Imagine a student just wants a quick list of options. They ask:\n",
    "> *\"What machine learning courses are available?\"*\n",
    "\n",
    "A human advisor would say: *\"We have CS001 (Intro to ML) and CS002 (Deep Learning).\"*\n",
    "\n",
    "Let's see what our Baseline Agent does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7c49dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the user's query\n",
    "query = \"What machine learning courses are available?\"\n",
    "\n",
    "print(f\"User asks: '{query}'\")\n",
    "print(\"Running workflow...\")\n",
    "\n",
    "# Run the graph!\n",
    "# We use .ainvoke() because our agent is async\n",
    "result = await workflow.ainvoke({\"query\": query})\n",
    "\n",
    "print(\"Workflow complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dc52b3",
   "metadata": {},
   "source": [
    "## Analysis: The Cost of \"Naive\" RAG\n",
    "\n",
    "The agent answered the question. But at what cost?\n",
    "\n",
    "Let's inspect the metrics returned in the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e73295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Answer\n",
    "print(\"=\"*60)\n",
    "print(f\"Agent Answer:\\n\\n{result['final_answer']}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Display the Metrics\n",
    "courses_found = result.get('courses_found', 0)\n",
    "total_tokens = result.get('total_tokens', 0)\n",
    "\n",
    "print(f\"\\nStatistics:\")\n",
    "print(f\"   Courses Retrieved: {courses_found}\")\n",
    "print(f\"   Total Tokens Used: {total_tokens:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b02bc2f",
   "metadata": {},
   "source": [
    "### Stop and Look\n",
    "\n",
    "Look at that Total Tokens number. It is likely over 6,000 tokens.\n",
    "\n",
    "For a simple question like *\"What courses are available?\"*, we used enough tokens to write a short essay.\n",
    "\n",
    "Why?\n",
    "Because the `research_node` retrieved the FULL details for every course it found. Let's peek at the `raw_context` that was sent to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7590d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the first 2000 characters of the context sent to the LLM\n",
    "raw_context = result.get('raw_context', '')\n",
    "\n",
    "print(f\"Total Context Size: {len(raw_context):,} characters\")\n",
    "print(\"-\" * 40)\n",
    "print(\"PREVIEW OF CONTEXT SENT TO LLM\")\n",
    "print(\"-\" * 40)\n",
    "print(raw_context[:2000] + \"\\n\\n... [TRUNCATED 20,000+ CHARACTERS] ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0649413",
   "metadata": {},
   "source": [
    "### The \"Needle in the Haystack\"\n",
    "\n",
    "Notice what is in that context:\n",
    "*   `\"week_1_topics\"`, `\"week_2_topics\"`... all the way to Week 14.\n",
    "*   `\"assignments\"`: Detailed lists of every homework.\n",
    "*   `\"grading_policy\"`: Breakdowns of percentages.\n",
    "\n",
    "The LLM didn't need ANY of this. It just needed the course titles and descriptions.\n",
    "\n",
    "### The Consequences\n",
    "1.  Financial Waste: You pay per token. 90% of these tokens were wasted.\n",
    "2.  Latency: Processing 6,000 tokens takes significantly longer than processing 500.\n",
    "3.  Distraction: When you flood the LLM with irrelevant data, it can \"hallucinate\" or get confused. It's like trying to find a phone number in a library by reading every book.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "We have successfully built a Baseline RAG Agent. It works, but it is inefficient and expensive.\n",
    "\n",
    "Key Takeaways:\n",
    "*   More is not always better. Retrieving full documents is rarely the right strategy.\n",
    "*   Context matters. We need to curate what we send to the LLM.\n",
    "\n",
    "### Next Lesson: Context Engineering\n",
    "In Stage 2, we will fix this. We will introduce Context Engineering techniques to:\n",
    "1.  Trim the data (remove nulls and empty fields).\n",
    "2.  Filter the data (only send summaries for the initial search).\n",
    "3.  Format the data (use clean markdown instead of raw JSON).\n",
    "\n",
    "See you in the next notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b5b585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Cleanup\n",
    "# If you want to clear the database, uncomment the lines below.\n",
    "# from agent import cleanup_courses\n",
    "# await cleanup_courses(course_manager)\n",
    "# print(\"Database cleaned.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
